{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TensorFlow Builder\u2019s Companion Book","text":"<p>Welcome to the TensorFlow Builder\u2019s Companion Book \u2014 your step-by-step guide to understanding the heart of TensorFlow through real-world reasoning, intuitive code walkthroughs, and progressively deeper system-level insights.</p> <p>This book is structured for learners, builders, and AI engineers who want to move beyond plug-and-play Keras models and gain mastery over <code>tf</code>, the core engine that powers TensorFlow.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>What <code>tf.Tensor</code> actually is and how TensorFlow represents computation.</li> <li>How to slice, reshape, broadcast, and train using tensors.</li> <li>A clear breakdown of every major API in TensorFlow, including:</li> <li><code>tf.Tensor</code>, <code>tf.Variable</code>, <code>tf.data</code>, <code>tf.GradientTape</code></li> <li><code>tf.keras</code> (Sequential, Functional, Subclassing APIs)</li> <li><code>tf.function</code>, <code>tf.math</code>, <code>tf.strings</code>, <code>tf.sparse</code>, and more.</li> <li>How TensorFlow handles training, backpropagation, optimization, and deployment.</li> <li>Real-world debugging, GPU support, and model lifecycle management.</li> </ul>"},{"location":"#who-this-book-is-for","title":"Who This Book Is For","text":"<ul> <li>Aspiring ML and deep learning engineers starting with TensorFlow.</li> <li>PyTorch users transitioning into the TensorFlow ecosystem.</li> <li>Developers and researchers building custom training loops and models.</li> <li>Anyone who wants to understand the how and why behind TensorFlow internals.</li> </ul>"},{"location":"#structure","title":"Structure","text":"<p>This book is organized in progressive layers, from fundamentals to deployment-ready applications. Each chapter is packed with:</p> <ul> <li>Human-friendly explanations  </li> <li>Annotated code samples  </li> <li>Common gotchas and TensorFlow quirks  </li> <li>Concept-to-practice connections  </li> <li>Optional links to Colab notebooks for hands-on learning</li> </ul> <p>Created and maintained by Clay Mark Sarte Built with \ud83d\udca1 curiosity and powered by TensorFlow graphs</p>"},{"location":"PartIII_overview/","title":"Part III Summary: Model Building with TensorFlow &amp; Keras","text":"<p>In Part III, we shift gears from understanding tensors and computation to actually building trainable models. This section teaches you how to construct, train, evaluate, and deploy neural networks using tf.keras, TensorFlow\u2019s high-level model API.</p> <p>Whether you\u2019re designing a simple feedforward neural network or preparing a model for production with callbacks and TensorBoard, this part forms the core skillset of any TensorFlow engineer.</p> <p>Here\u2019s what you\u2019ll master across Chapters 13 to 20:  </p> <p>\u2705 Chapter 13: TensorFlow Keras API \u2013 Anatomy of a Model  </p> <p>You\u2019ll explore how tf.keras is structured and learn three different ways to build models: Sequential, Functional, and Subclassing. This chapter is the gateway into deep learning architecture\u2014laying the foundation for everything that follows.</p> <p>\u2705 Chapter 14: Building a Neural Network from Scratch</p> <p>Get hands-on with your first end-to-end model: from layer initialization to compiling and training it on actual data. You'll apply everything from Chapter 13 and feel what it\u2019s like to breathe life into a neural net.</p> <p>\u2705 Chapter 15: Layers &amp; Activation Functions</p> <p>Understand the different layer types (Dense, Conv2D, Dropout, etc.) and how to design them effectively. This chapter also dives into activation functions like ReLU, Sigmoid, and Softmax\u2014and when to use them.</p> <p>\u2705 Chapter 16: Loss Functions &amp; Optimizers</p> <p>Explore the mathematical functions that guide your model\u2019s learning. You\u2019ll learn when to use categorical crossentropy vs MSE, and how optimizers like Adam, SGD, and RMSprop affect convergence.</p> <p>\u2705 Chapter 17: Backpropagation &amp; Gradient Descent</p> <p>Peek under the hood of how training actually works. This chapter demystifies backpropagation, how gradients flow through layers, and how loss and weight updates are calculated\u2014bridging theory and TensorFlow code.</p> <p>\u2705 Chapter 18: Model Training with fit() and evaluate()</p> <p>This is where the magic happens: you\u2019ll learn how to use Keras\u2019s fit(), evaluate(), and predict() to handle training loops, batch sizes, and validation data\u2014all with minimal code.</p> <p>\u2705 Chapter 19: Saving, Loading, and Callbacks</p> <p>Preserve your model with model.save(), reload it for inference, and customize training with callbacks like EarlyStopping, ModelCheckpoint, and LearningRateScheduler\u2014tools every serious model needs.</p> <p>\u2705 Chapter 20: Visualizing Model Progress with TensorBoard</p> <p>You\u2019ll learn how to monitor model training in real time using TensorBoard, TensorFlow\u2019s built-in visualization tool. This chapter introduces logs, scalars, histograms, and graphs for debugging and performance tracking.</p> <p>After Part III, You Will Be Able To:</p> <ul> <li> <p>Build models of increasing complexity, from scratch  </p> </li> <li> <p>Choose the right architecture, optimizer, and loss function  </p> </li> <li> <p>Train efficiently and debug performance issues  </p> </li> <li> <p>Save and serve your models  </p> </li> <li> <p>Visualize learning in real time with TensorBoard  </p> </li> </ul> <p>Part III transforms you from a Tensor user into a TensorFlow builder.</p>"},{"location":"PartII_overview/","title":"Part II: Tensor Mechanics and Computation","text":"<p>\u201cBefore models can learn, data must be shaped, sliced, and understood.\u201d</p> <p>In Part II, you move from simply knowing what a tensor is to mastering how to manipulate, structure, and compute with them efficiently. This section gives you the foundational mechanics required to build, debug, and optimize any deep learning pipeline.</p> <p>These chapters are not flashy\u2014but they\u2019re absolutely essential. Every model, from CNNs to Transformers, starts with clean tensor ops under the hood.</p> <p>\u2705 Chapter 6: Tensor Indexing &amp; Reshaping</p> <p>You\u2019ll learn how to slice and reshape tensors like a ninja: selecting rows, flattening shapes, adding batch dimensions, and preparing inputs for layers.</p> <p>\u2705 Chapter 7: Tensor Broadcasting</p> <p>One of TensorFlow\u2019s most elegant features. This chapter demystifies how tensors of different shapes are automatically aligned and expanded during element-wise operations\u2014no boilerplate reshaping required.</p> <p>\u2705 Chapter 8: Ragged, Sparse, and String Tensors</p> <p>Not all tensors are pretty. You\u2019ll work with jagged text sequences (ragged), memory-efficient structures (sparse), and string tensors for natural language processing\u2014crucial for real-world data.</p> <p>\u2705 Chapter 9: Variables &amp; Trainable Parameters</p> <p>Meet tf.Variable, the mutable tensor that learns. You'll explore how TensorFlow stores, updates, and tracks parameters across training steps.</p> <p>\u2705 Chapter 10: Automatic Differentiation (tf.GradientTape)</p> <p>This is the engine behind learning. You'll record forward passes and compute gradients for optimization using TensorFlow's autodiff system\u2014step by step.</p> <p>\u2705 Chapter 11: Graphs &amp; Functions (@tf.function)</p> <p>Performance mode, engaged. You\u2019ll convert Python into TensorFlow graphs, improving speed and enabling deployment to GPUs, TPUs, and production systems.</p> <p>\u2705 Chapter 12: Bitwise &amp; Numerical Operations</p> <p>The finishing move. Learn advanced operations like clipping, rounding, modulo, one-hot encoding, and bitwise ops\u2014used for loss design, masking, and precision tuning.</p> <p>After Part II, You Will Be Able To:</p> <ul> <li> <p>Slice, reshape, and broadcast tensors confidently</p> </li> <li> <p>Work with variable-length and sparse data</p> </li> <li> <p>Write training loops that compute and apply gradients</p> </li> <li> <p>Build efficient TensorFlow graphs for speed</p> </li> <li> <p>Understand the math operations that power model internals</p> </li> </ul> <p>Part II gives you the raw power of TensorFlow. Part III shows you how to channel it into models.</p>"},{"location":"PartIV_overview/","title":"Part IV Summary: Natural Language Processing with TensorFlow","text":"<p>In Part IV, we shift from numeric tensors to language understanding. This section explores how TensorFlow handles text\u2014turning characters and sentences into meaningful vectors that machines can process.</p> <p>From classic NLP methods like Bag-of-Words and TF-IDF, to advanced architectures like LSTMs and Transformers, this part teaches you to build systems that interpret and generate language.</p> <p>Here\u2019s what you\u2019ll master across Chapters 21 to 25:</p> <p>\u2705 Chapter 21: Text Preprocessing &amp; Tokenization Learn how to clean, tokenize, and vectorize raw text using TensorFlow\u2019s TextVectorization layer and Keras\u2019s Tokenizer API. You\u2019ll prepare sequence data for embedding layers and deep NLP models.</p> <p>\u2705 Chapter 22: TF-IDF, Bag-of-Words Representations Explore foundational NLP techniques\u2014count-based and frequency-based vectorization. Use CountVectorizer and TfidfVectorizer from scikit-learn to create interpretable, fast baselines for text classification tasks.</p> <p>\u2705 Chapter 23: RNNs &amp; LSTMs Dive into Recurrent Neural Networks and Long Short-Term Memory (LSTM) layers for learning from sequential text. You\u2019ll build a sentiment classifier and understand how memory helps capture temporal context.</p> <p>\u2705 Chapter 24: Transformers in TensorFlow (from Scratch) Discover the self-attention mechanism and build a mini Transformer Encoder using pure TensorFlow. This chapter provides an inside-out understanding of what powers BERT, GPT, and modern NLP architectures.</p> <p>\u2705 Chapter 25: NLP Projects \u2013 Spam Detection, Sentiment Analysis, Autocomplete Apply everything you\u2019ve learned in three mini-projects that span traditional and deep learning workflows. You\u2019ll build and deploy real models that understand, classify, and even complete natural language.</p> <p>After Part IV, You Will Be Able To:</p> <ul> <li> <p>Preprocess text and convert it to model-ready tensors</p> </li> <li> <p>Choose the right representation (TF-IDF, embeddings, etc.) for your task</p> </li> <li> <p>Build sequence models using RNNs, LSTMs, and Transformers</p> </li> <li> <p>Create and deploy end-to-end NLP systems like sentiment analyzers or spam filters</p> </li> <li> <p>Understand the inner workings of attention-based architectures</p> </li> </ul> <p>Part IV turns raw text into signals of meaning\u2014paving the way for intelligent systems that speak, understand, and respond.</p>"},{"location":"PartI_overview/","title":"Part I: Understanding TensorFlow Fundamentals","text":"<p>\u201cYou can\u2019t build AI until you understand the engine that powers it.\u201d</p> <p>Part I lays the foundation for everything you\u2019ll do with TensorFlow. Whether you\u2019re building a neural network, deploying to mobile, or diving into transformers, it all starts here\u2014with understanding what TensorFlow actually is, how it works under the hood, and how to set it up properly.</p> <p>By the end of this section, you\u2019ll know how TensorFlow thinks, and how to speak its language fluently\u2014starting with tensors, variables, and graphs.</p> <p>\u2705 Chapter 1: What is TensorFlow? An introduction to the TensorFlow ecosystem, its purpose, and its core components. You\u2019ll learn what makes it different from other frameworks and why it\u2019s more than just a library\u2014it's a complete ML platform.</p> <p>\u2705 Chapter 2: Architecture of TensorFlow A deeper look into how TensorFlow works internally. You'll explore computation graphs, eager execution, the gradient engine, the deployment stack (TFLite, TFX), and how everything fits together.</p> <p>\u2705 Chapter 3: TensorFlow vs Keras Not all Keras is TensorFlow\u2014and not all TensorFlow needs Keras. This chapter clarifies the relationship between them, compares standalone Keras with tf.keras, and shows which one to use when (spoiler: it's tf.keras).</p> <p>\u2705 Chapter 4: Installing TensorFlow (Windows, Linux, MacOS) You\u2019ll learn how to properly install TensorFlow with or without GPU support, how to create virtual environments, and how to verify CUDA and device configuration. Practical, essential, and clean.</p> <p>\u2705 Chapter 5: First Tensor Example \u2013 Hello, tf.Tensor! This is your \u201chello world\u201d moment. You\u2019ll learn how to create tensors, inspect them, perform basic math, and understand shapes, ranks, and types. It's the first hands-on look at how TensorFlow represents and manipulates data.</p> <p>After Part I, You Will Be Able To:</p> <ul> <li> <p>Understand the role and scope of TensorFlow in machine learning  </p> </li> <li> <p>Differentiate between TensorFlow and Keras (and when to use each)  </p> </li> <li> <p>Set up your local dev environment (with GPU acceleration)  </p> </li> <li> <p>Create and manipulate basic tensors with confidence  </p> </li> <li> <p>Prepare for deeper topics like training models and computing gradients  </p> </li> </ul> <p>Part I is where the groundwork is laid. The rest of the book builds upon it\u2014one tensor at a time.</p>"},{"location":"PartVI_overview/","title":"Part VI Summary: Real-World Applications","text":"<p>While the earlier parts taught us how to build models, Part VI is where we bring everything to life.</p> <p>This section focuses on deployment, scaling, and integration\u2014the critical steps required to turn deep learning prototypes into full-fledged, real-world solutions. You\u2019ll explore how to forecast stock prices, recommend movies, optimize TensorFlow for mobile devices, deploy models in production pipelines, and even bridge TensorFlow with Hugging Face\u2019s powerful NLP models.</p> <p>This is TensorFlow beyond notebooks\u2014into products.</p> <p>Here\u2019s what you\u2019ll master across Chapters 33 to 37:</p> <p>\u2705 Chapter 33: Time Series Forecasting Learn how to work with sequential data to predict future values using RNNs, LSTMs, and 1D CNNs. You\u2019ll explore windowing techniques, lag features, and real-world use cases like temperature, sales, or stock price forecasting.</p> <p>\u2705 Chapter 34: Recommender Systems Build systems that personalize content for users\u2014like Netflix suggestions or Amazon product feeds. You\u2019ll learn collaborative filtering, matrix factorization, and TensorFlow Recommenders (TFRS) to create end-to-end pipelines.</p> <p>\u2705 Chapter 35: TensorFlow Lite &amp; Mobile Deployment Deploy models to edge devices like smartphones and microcontrollers. Learn how to convert models to .tflite, apply quantization, and integrate them into Android/iOS apps for efficient, low-latency inference.</p> <p>\u2705 Chapter 36: TensorFlow Extended (TFX) for Production Pipelines Master the components of a production ML pipeline: data validation, preprocessing, training, evaluation, and deployment. You\u2019ll design robust, scalable systems using TFX tools like ExampleGen, Trainer, Pusher, and TFServing.</p> <p>\u2705 Bonus Chapter 37: Integrating TensorFlow with Hugging Face Supercharge your TensorFlow projects with Hugging Face\u2019s vast transformer model hub. From using transformers in TensorFlow pipelines to exporting models between PyTorch and TF, this chapter shows you how to build hybrid NLP-vision systems.</p> <p>After Part VI, You Will Be Able To:</p> <ul> <li>Apply deep learning to time series and user personalization problems</li> <li>Convert and optimize models for mobile and edge deployment</li> <li>Build end-to-end production ML pipelines using TFX</li> <li>Integrate TensorFlow models with other powerful ecosystems like Hugging Face</li> <li>Deploy AI models as real-world products</li> </ul> <p>Part VI transforms your TensorFlow knowledge into engineering impact\u2014where models don't just learn, but serve, scale, and ship.</p>"},{"location":"PartV_overview/","title":"Part V Summary: Computer Vision in TensorFlow","text":"<p>Part V takes us from sequences of words to arrays of pixels. In this section, we explore how machines can see, interpret, and generate images using TensorFlow\u2019s deep learning tools.</p> <p>Computer Vision is one of the most impactful applications of AI\u2014spanning everything from photo tagging and medical imaging to self-driving cars and generative art. This part guides you through the essential building blocks of vision systems, using both classic convolutional networks and cutting-edge techniques like GANs.</p> <p>Here\u2019s what you\u2019ll master across Chapters 26 to 32:</p> <p>\u2705 Chapter 26: Convolution Layers &amp; CNNs Understand how convolutional layers work and why they\u2019re ideal for spatial data like images. You\u2019ll build your first Convolutional Neural Network (CNN) from scratch and visualize how filters extract features like edges and textures.</p> <p>\u2705 Chapter 27: Data Augmentation Boost your model\u2019s generalization by simulating real-world variations through flipping, rotation, zooming, and brightness changes. You\u2019ll explore TensorFlow\u2019s image preprocessing pipeline and the tf.image and ImageDataGenerator APIs.</p> <p>\u2705 Chapter 28: Image Classification Train a CNN to classify images into categories using popular datasets like CIFAR-10 or MNIST. This chapter walks you through the full pipeline\u2014from loading data to evaluating your model\u2019s performance on test images.</p> <p>\u2705 Chapter 29: Object Detection Go beyond classification and detect what and where objects are in an image. You\u2019ll explore bounding boxes, Intersection over Union (IoU), and modern object detection models like SSD and YOLO in TensorFlow.</p> <p>\u2705 Chapter 30: Face Mask Detection Build a real-world application that classifies whether a person is wearing a mask using transfer learning. You'll leverage pre-trained models like MobileNetV2 and fine-tune them for COVID-era use cases.</p> <p>\u2705 Chapter 31: Image Segmentation Segment images into pixel-level classes with models like U-Net or DeepLab. You'll learn the difference between semantic and instance segmentation\u2014and how to label and train on pixel masks.</p> <p>\u2705 Chapter 32: GANs for Image Generation Explore the magical world of Generative Adversarial Networks. Learn how a generator and a discriminator play a zero-sum game to create photorealistic images. You'll build a simple GAN using TensorFlow and watch it learn to generate from noise.</p> <p>After Part V, You Will Be Able To:</p> <ul> <li>Build and train CNNs for various vision tasks  </li> <li>Preprocess and augment image datasets for improved model robustness  </li> <li>Perform image classification, object detection, and segmentation  </li> <li>Use transfer learning for faster and more accurate vision models  </li> <li>Generate entirely new images using GANs</li> </ul> <p>Part V teaches machines to see\u2014not just recognize, but detect, segment, and create. Welcome to the visual intelligence era.</p>"},{"location":"appendices/","title":"Appendices","text":"<p>\u201cThe end of the book is the beginning of mastery.\u201d</p> <p>These appendices are designed to serve as your quick-reference toolkit and troubleshooting companion as you build real-world TensorFlow projects.</p>"},{"location":"appendices/#appendix-a-tensor-shapes-cheat-sheet-tensorflow-style","title":"Appendix A: Tensor Shapes Cheat Sheet (TensorFlow Style)","text":"Shape Notation Meaning Example <code>(batch_size,)</code> 1D vector (e.g., labels) <code>[32]</code> <code>(batch_size, features)</code> 2D input (e.g., tabular data) <code>[32, 10]</code> <code>(batch_size, height, width, channels)</code> Image input (NHWC format) <code>[32, 28, 28, 1]</code> <code>(batch_size, time_steps, features)</code> Sequences (e.g., RNN input) <code>[32, 100, 64]</code> <code>(vocab_size, embedding_dim)</code> Word embeddings <code>[10000, 300]</code> <p>\ud83d\udca1 Did you know? TensorFlow prefers NHWC format for convolutional layers: [batch, height, width, channels], while PyTorch uses NCHW by default.</p>"},{"location":"appendices/#appendix-b-tensorflow-vs-pytorch-code-comparison","title":"Appendix B: TensorFlow vs PyTorch \u2013 Code Comparison","text":"Task TensorFlow (Keras) PyTorch Dense Layer <code>tf.keras.layers.Dense(128)</code> <code>nn.Linear(in, out)</code> Activation <code>activation='relu'</code> <code>F.relu(x)</code> Loss Function <code>SparseCategoricalCrossentropy()</code> <code>nn.CrossEntropyLoss()</code> Optimizer <code>Adam(learning_rate=1e-3)</code> <code>optim.Adam(model.parameters())</code> Training Loop <code>model.fit(x, y)</code> <code>for epoch in ...: optimizer.step()</code> Model Definition Subclass <code>tf.keras.Model</code> Subclass <code>nn.Module</code>"},{"location":"appendices/#appendix-c-debugging-tips-for-tensorflow","title":"Appendix C: Debugging Tips for TensorFlow","text":"Issue Cause Fix <code>Graph execution error</code> Mixing eager + graph mode Use <code>@tf.function</code> carefully <code>ValueError: Shapes (None, 1) != (None, )</code> Shape mismatch Check model output vs label shape Model not learning Wrong loss, optimizer, or learning rate Try <code>lr=1e-3</code> and experiment GPU not being used Device misconfiguration Use <code>tf.config.list_physical_devices('GPU')</code> Out-of-memory error (OOM) Batch size too large or model too big Reduce batch size or use mixed precision"},{"location":"appendices/#appendix-e-full-api-reference-crosswalk","title":"\ud83d\uddc2 Appendix E: Full API Reference Crosswalk","text":"<p>\u201cKnowing what\u2019s possible is the first step to mastery.\u201d</p> <p>This appendix maps the most essential TensorFlow API classes and functions to their official documentation links, along with a quick summary of when and why you\u2019d use each.</p> Module API Summary Docs tf.Tensor tf.constant, tf.Variable Core tensor creation. Use constants for fixed data, variables for trainable parameters.    tf.Tensor tf.tensor tf.keras Sequential, Model, Layer Core model-building classes. Sequential for stacks, subclass Model for custom. tf.keras tf.keras tf.keras.layers Dense, Conv2D, LSTM, Flatten, Dropout Building blocks of neural networks: fully connected, CNN, RNN, etc.    tf.keras.layers tf.keras.layers tf.keras.losses CategoricalCrossentropy, MSE, Huber  Standard loss functions for classification, regression, and robust fitting. tf.keras.losses tf.keras.losses tf.keras.optimizers Adam, SGD, RMSprop Optimizers for gradient descent-based training.    tf.keras.optimizers tf.keras.optimizers tf.data Dataset.from_tensor_slices, .batch(), .shuffle() Efficient, scalable data pipelines for training.   tf.data tf.data tf.image resize, random_flip, per_image_standardization Image preprocessing utilities (augmentation, normalization).   tf.image tf.image tf.function @tf.function Converts Python code into efficient graph-based TensorFlow execution.  tf.function tf.function tf.GradientTape tf.GradientTape() Enables automatic differentiation for custom training loops.   tf.GradientTape tf.GradientTape tf.lite TFLiteConverter, Interpreter Tools to convert and run models on mobile/embedded devices.    tf.lite tf.lite tensorflow_recommenders tfrs.Model, tfrs.tasks.Retrieval TensorFlow Recommenders for personalized ranking and retrieval.    TensorFlow Recommenders TensorFlow Recommmenders transformers (HF) TFAutoModel, AutoTokenizer, pipeline() Hugging Face Transformers (TF version) for NLP &amp; vision models.    Hugging Face Transformers Hugging Face Transformers tfx ExampleGen, Trainer, Pusher, Evaluator TensorFlow Extended pipeline components for production ML. TFX TFX tensorboard tf.summary, TensorBoard callback Logs scalars, images, and graphs during training.  TensorBoard TensorBoard"},{"location":"appendices/#pro-tip-you-can-quickly-access-any-tensorflow-documentation-by-appending-the-api-name-to","title":"\ud83d\udca1 Pro Tip: You can quickly access any TensorFlow documentation by appending the API name to:","text":"<p>\ud83d\udcce https://www.tensorflow.org/api_docs/python/tf/YourAPIHere</p>"},{"location":"chapter10_automatic_differentiation/","title":"Chapter 10: Automatic Differentiation <code>(tf.GradientTape)</code>","text":"<p>\u201cThe power to learn lies not in the function\u2014but in how it changes.\u201d</p>"},{"location":"chapter10_automatic_differentiation/#101-what-is-automatic-differentiation","title":"10.1 What Is Automatic Differentiation?","text":"<p>Automatic differentiation (autodiff) is the core engine of backpropagation. It allows TensorFlow to:</p> <ul> <li>Record operations as a computation graph  </li> <li>Compute gradients w.r.t. any variable  </li> <li>Use those gradients to optimize parameters  </li> </ul> <p>In TensorFlow, this is done using <code>tf.GradientTape</code>.</p>"},{"location":"chapter10_automatic_differentiation/#102-recording-with-tfgradienttape","title":"10.2 Recording with <code>tf.GradientTape</code>","text":"<pre><code>import tensorflow as tf\n\nx = tf.Variable(3.0)\n\nwith tf.GradientTape() as tape:\n    y = x**2 + 2*x + 1  # y = (x + 1)^2\n\ndy_dx = tape.gradient(y, x)\nprint(\"dy/dx:\", dy_dx.numpy())  # Should be 2x + 2 \u2192 8.0\n</code></pre> <p>By default, tape only watches <code>tf.Variables</code>.</p>"},{"location":"chapter10_automatic_differentiation/#103-watching-non-variable-tensors","title":"10.3 Watching Non-Variable Tensors","text":"<p>If you want to differentiate w.r.t. a tensor (not a variable): <pre><code>x = tf.constant(3.0)\n\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = x**2\n\ndy_dx = tape.gradient(y, x)\nprint(\"dy/dx:\", dy_dx.numpy())  # 6.0\n</code></pre></p>"},{"location":"chapter10_automatic_differentiation/#104-multivariate-gradients","title":"10.4 Multivariate Gradients","text":"<p><pre><code>x = tf.Variable(1.0)\ny = tf.Variable(2.0)\n\nwith tf.GradientTape() as tape:\n    f = x**2 + y**3\n\ngrads = tape.gradient(f, [x, y])\nprint(\"df/dx:\", grads[0].numpy())  # 2x \u2192 2\nprint(\"df/dy:\", grads[1].numpy())  # 3y^2 \u2192 12\n</code></pre> You can compute gradients for multiple variables at once\u2014essential in model training.</p>"},{"location":"chapter10_automatic_differentiation/#105-persistent-tapes-reuse-for-multiple-gradients","title":"10.5 Persistent Tapes (Reuse for Multiple Gradients)","text":"<p><pre><code>x = tf.Variable(1.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    y = x**2\n    z = x**3\n\ndy_dx = tape.gradient(y, x)  # 2x\ndz_dx = tape.gradient(z, x)  # 3x^2\nprint(dy_dx.numpy(), dz_dx.numpy())  # 2.0, 3.0\n\ndel tape  # Cleanup\n</code></pre> Use <code>persistent=True</code> if you need to compute multiple gradients from the same tape.</p>"},{"location":"chapter10_automatic_differentiation/#106-using-gradients-for-optimization","title":"10.6 Using Gradients for Optimization","text":"<p><pre><code>x = tf.Variable(2.0)\n\nlearning_rate = 0.1\nfor i in range(10):\n    with tf.GradientTape() as tape:\n        loss = (x - 5)**2\n\n    grad = tape.gradient(loss, x)\n    x.assign_sub(learning_rate * grad)\n\n    print(f\"Step {i}, x: {x.numpy():.4f}, loss: {loss.numpy():.4f}\")\n</code></pre> This mimics gradient descent\u2014updating x to minimize the loss.</p>"},{"location":"chapter10_automatic_differentiation/#107-summary","title":"10.7 Summary","text":"<ul> <li>tf.GradientTape is TensorFlow\u2019s way of recording operations and computing gradients automatically.</li> <li>It\u2019s the core of learning\u2014used in every optimizer.</li> <li>Gradients are computed via tape.gradient(loss, variables)</li> <li>You can track multiple variables, persist the tape, or manually apply updates.</li> </ul> <p>\u201cThe power to learn lies not in the function\u2014but in how it changes.\u201d</p>"},{"location":"chapter11_graphs_functions/","title":"Chapter 11: Graphs &amp; Functions (@tf.function)","text":"<p>\u201cFirst you write Python. Then you write TensorFlow. Then you make Python act like TensorFlow.\u201d</p>"},{"location":"chapter11_graphs_functions/#111-why-graphs","title":"11.1 Why Graphs?","text":"<p>Python is great for prototyping. But it\u2019s slow when you scale to: - Production inference - Deployment across devices - Multi-GPU/TPU execution - Model export for TFLite or TensorFlow Serving</p> <p>TensorFlow solves this with graphs: dataflow representations that can be optimized and executed outside the Python runtime.</p>"},{"location":"chapter11_graphs_functions/#112-enter-tffunction","title":"11.2 Enter @tf.function","text":"<p>You write regular Python\u2014but TensorFlow traces it once and converts it into a graph.</p> <p>\u2705 Example: <pre><code>import tensorflow as tf\n\n@tf.function\ndef compute(x):\n    return x**2 + 3*x + 1\n\nprint(compute(tf.constant(2.0)))  # tf.Tensor(11.0, shape=(), dtype=float32)\n</code></pre> This runs like Python but is compiled under the hood.</p>"},{"location":"chapter11_graphs_functions/#113-benefits-of-using-tffunction","title":"11.3 Benefits of Using @tf.function","text":"<ul> <li>Faster execution: Runs as a graph instead of interpreted Python  </li> <li>Cross-platform compatibility: Can run on GPUs, TPUs, mobile, etc.  </li> <li>Serialization: Enables saving models in SavedModel format  </li> <li>Deployment: Used in TensorFlow Serving, TFLite, and TF.js</li> </ul>"},{"location":"chapter11_graphs_functions/#114-gotchas-debugging-tips","title":"11.4 Gotchas &amp; Debugging Tips","text":"<p>\u2757 Be careful of Python-side effects: <pre><code>@tf.function\ndef bad_func():\n    print(\"This won't show up\")  # Runs only during tracing, not each call\n</code></pre> Only TensorFlow ops are tracked. Use <code>tf.print()</code> instead of Python <code>print()</code>: <pre><code>@tf.function\ndef good_func(x):\n    tf.print(\"Value of x:\", x)\n</code></pre></p>"},{"location":"chapter11_graphs_functions/#115-input-signatures-optional","title":"11.5 Input Signatures (Optional)","text":"<p>Restrict the function to a fixed input type and shape for optimization: <pre><code>@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\ndef model(x):\n    return tf.reduce_sum(x)\n</code></pre> This makes tracing more predictable and speeds up model serving.</p>"},{"location":"chapter11_graphs_functions/#116-retrieving-the-graph","title":"11.6 Retrieving the Graph","text":"<p>You can inspect the computation graph like this: <pre><code>@tf.function\ndef square(x):\n    return x * x\n\nprint(square.get_concrete_function(tf.constant(2.0)).graph.as_graph_def())\n</code></pre> This shows you the internal graph ops (used for debugging, export, or tooling).</p>"},{"location":"chapter11_graphs_functions/#117-common-use-cases","title":"11.7 Common Use Cases","text":"Use Case Benefit Training loops Speed boost Model export (SavedModel) Required TF Serving / deployment Required Writing reusable pipelines Cleaner graph structure"},{"location":"chapter11_graphs_functions/#118-summary","title":"11.8 Summary","text":"<ul> <li>@tf.function converts Python code into high-performance TensorFlow graphs.  </li> <li>It enables speed, portability, deployment, and tracing.  </li> <li>Use tf.print instead of regular print() inside graph code.  </li> <li>It\u2019s a must for production workflows, but test in eager mode first.  </li> </ul> <p>\u201cFirst you write Python. Then you write TensorFlow. Then you make Python act like TensorFlow.\u201d</p>"},{"location":"chapter12_bitwise_numerical_ops/","title":"Chapter 12: Bitwise &amp; Numerical Operations","text":"<p>\u201cMachine learning is built on numbers\u2014but sharpened with operations.\u201d</p>"},{"location":"chapter12_bitwise_numerical_ops/#121-why-these-ops-matter","title":"12.1 Why These Ops Matter","text":"<p>These operations may seem low-level, but they power:  </p> <ul> <li>Loss functions  </li> <li>Data normalization  </li> <li>Activation functions  </li> <li>Efficient GPU/TPU processing  </li> <li>Feature engineering &amp; logical masking  </li> </ul> <p>If you want full control over your data pipeline or model internals, this is your toolkit.</p>"},{"location":"chapter12_bitwise_numerical_ops/#122-numerical-operations","title":"12.2 Numerical Operations","text":"<p>TensorFlow supports a full range of math operations:</p>"},{"location":"chapter12_bitwise_numerical_ops/#element-wise-math","title":"\u2705 Element-wise Math:","text":"<pre><code>x = tf.constant([1.0, 2.0, 3.0])\n\nprint(tf.math.square(x))     # [1, 4, 9]\nprint(tf.math.sqrt(x))       # [1.0, 1.4142, 1.7320]\nprint(tf.math.exp(x))        # Exponential\nprint(tf.math.log(x))        # Natural log\n</code></pre>"},{"location":"chapter12_bitwise_numerical_ops/#reduction-ops","title":"\u2705 Reduction Ops:","text":"<pre><code>matrix = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n\nprint(tf.reduce_sum(matrix))           # 10.0\nprint(tf.reduce_mean(matrix))          # 2.5\nprint(tf.reduce_max(matrix, axis=0))   # [3.0, 4.0]\n</code></pre> <p>\ud83d\udca1 Reduction ops collapse tensors along a specified axis.</p>"},{"location":"chapter12_bitwise_numerical_ops/#123-rounding-clipping","title":"12.3 Rounding &amp; Clipping","text":"<pre><code>a = tf.constant([1.2, 2.5, 3.8])\n\nprint(tf.round(a))       # [1.0, 2.0, 4.0]\nprint(tf.floor(a))       # [1.0, 2.0, 3.0]\nprint(tf.math.ceil(a))   # [2.0, 3.0, 4.0]\nprint(tf.clip_by_value(a, 1.5, 3.0))  # [1.5, 2.5, 3.0]\n</code></pre>"},{"location":"chapter12_bitwise_numerical_ops/#124-bitwise-operations-for-integers-only","title":"12.4 Bitwise Operations (for Integers Only)","text":"<p>Bitwise operations are useful for: - Masks and binary logic - Pixel manipulation (in image tasks) - Efficient boolean filters</p> <pre><code>x = tf.constant([0b1010, 0b1100], dtype=tf.int32)\ny = tf.constant([0b0101, 0b1010], dtype=tf.int32)\n\nprint(tf.bitwise.bitwise_and(x, y))  # [0b0000, 0b1000]\nprint(tf.bitwise.bitwise_or(x, y))   # [0b1111, 0b1110]\nprint(tf.bitwise.invert(x))          # Bitwise NOT\nprint(tf.bitwise.left_shift(x, 1))   # Shift left (\u00d72)\nprint(tf.bitwise.right_shift(x, 1))  # Shift right (\u00f72)\n</code></pre>"},{"location":"chapter12_bitwise_numerical_ops/#125-modulo-and-sign","title":"12.5 Modulo and Sign","text":"<p><pre><code>print(tf.math.mod(17, 5))         # 2\nprint(tf.math.floormod(17, 5))    # 2\nprint(tf.math.sign([-2.0, 0.0, 3.0]))  # [-1.0, 0.0, 1.0]\n</code></pre> These are useful in:  </p> <ul> <li>Cycle detection  </li> <li>Position encoding  </li> <li>Boolean masks for data pipelines</li> </ul>"},{"location":"chapter12_bitwise_numerical_ops/#126-one-hot-encoding-bonus","title":"12.6 One-Hot Encoding (Bonus)","text":"<pre><code>labels = tf.constant([0, 2, 1])\none_hot = tf.one_hot(labels, depth=3)\nprint(one_hot)\n</code></pre>"},{"location":"chapter12_bitwise_numerical_ops/#output","title":"Output:","text":"<p><pre><code>[[1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]]\n</code></pre> This is crucial for classification tasks before training.</p>"},{"location":"chapter12_bitwise_numerical_ops/#127-summary","title":"12.7 Summary","text":"<ul> <li>TensorFlow supports a wide range of numerical, reduction, and bitwise operations.  </li> <li>These ops form the foundation for loss computation, feature preprocessing, and low-level tensor control.  </li> <li>Mastering them helps you go beyond layers\u2014into the math powering them.</li> </ul> <p>\u201cMachine learning is built on numbers\u2014but sharpened with operations.\u201d</p>"},{"location":"chapter12_bitwise_numerical_ops/#end-of-part-ii-tensor-mechanics-and-computation","title":"End of Part II: Tensor Mechanics and Computation","text":"<p>You now know how to:  </p> <ul> <li>Slice, reshape, and broadcast tensors  </li> <li>Work with ragged, sparse, and string data  </li> <li>Create trainable variables  </li> <li>Record and compute gradients  </li> <li>Write high-performance TensorFlow graphs</li> </ul>"},{"location":"chapter13_API/","title":"Chapter 13: TensorFlow Keras API \u2013 Anatomy of a Model","text":"<p>\u201cA model is just an idea\u2014until it gets layers, weights, and shape.\u201d</p>"},{"location":"chapter13_API/#131-what-is-tfkeras","title":"13.1 What is <code>tf.keras</code>?","text":"<p><code>tf.keras</code> is TensorFlow\u2019s official high-level API for building, training, and deploying machine learning models.</p> <p>It's designed to be:  </p> <ul> <li>User-friendly (simple syntax)  </li> <li>Modular (layers, optimizers, callbacks)  </li> <li>Extensible (custom layers/models)  </li> <li>Integrated (with TensorFlow ecosystem)  </li> </ul> <p>Keras wraps the complexity of TensorFlow so you can focus on structure and logic, not boilerplate.</p>"},{"location":"chapter13_API/#132-the-3-model-building-styles","title":"13.2 The 3 Model Building Styles","text":"<p>There are three ways to build models using <code>tf.keras</code>:</p>"},{"location":"chapter13_API/#1-sequential-api-beginner-friendly","title":"\u2705 1. Sequential API (Beginner-friendly)","text":"<pre><code>from tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(100,)),\n    layers.Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"chapter13_API/#2-functional-api-flexible-architectures","title":"\u2705 2. Functional API (Flexible architectures)","text":"<pre><code>inputs = tf.keras.Input(shape=(100,))\nx = layers.Dense(64, activation='relu')(inputs)\noutputs = layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"chapter13_API/#3-subclassing-api-for-full-control","title":"\u2705 3. Subclassing API (For full control)","text":"<p><pre><code>class MyModel(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = layers.Dense(64, activation='relu')\n        self.out = layers.Dense(10, activation='softmax')\n\n    def call(self, x):\n        x = self.dense1(x)\n        return self.out(x)\n\nmodel = MyModel()\n</code></pre> Each has trade-offs. Start with Sequential, move to Functional for branching inputs/outputs, and use Subclassing for full customization.</p>"},{"location":"chapter13_API/#133-anatomy-of-a-keras-model","title":"13.3 Anatomy of a Keras Model","text":"<p>Here\u2019s what makes up a model under the hood:</p> Component Description Input Layer Defines the shape of input data Hidden Layers The intermediate processing units Output Layer Final layer for predictions Loss Function Measures model\u2019s error Optimizer Updates weights based on gradients Metrics Monitors performance (accuracy, loss, etc.)"},{"location":"chapter13_API/#134-model-compilation","title":"13.4 Model Compilation","text":"<p><pre><code>model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n</code></pre> This sets the training configuration, including how the model learns and what it tracks.</p>"},{"location":"chapter13_API/#135-summary-of-a-simple-model-lifecycle","title":"13.5 Summary of a Simple Model Lifecycle","text":"<pre><code># 1. Build the model\nmodel = tf.keras.Sequential([\n    layers.Dense(128, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\n# 2. Compile\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# 3. Train\nmodel.fit(x_train, y_train, epochs=5)\n\n# 4. Evaluate\nmodel.evaluate(x_test, y_test)\n\n# 5. Predict\npreds = model.predict(x_new)\n</code></pre>"},{"location":"chapter13_API/#136-summary","title":"13.6 Summary","text":"<ul> <li>tf.keras is TensorFlow\u2019s high-level API for model building.  </li> <li>You can build models using Sequential, Functional, or Subclassing styles.  </li> <li>Models have layers, losses, optimizers, and metrics\u2014all handled cleanly.  </li> <li>Knowing the anatomy helps you debug, customize, and scale efficiently.  </li> </ul> <p>\u201cA model is just an idea\u2014until it gets layers, weights, and shape.\u201d</p>"},{"location":"chapter14_nn_from_scratch/","title":"Chapter 14: Building a Neural Network from Scratch","text":"<p>\u201cBefore you rely on magic, understand the machinery beneath it.\u201d</p> <p>In this chapter, we'll strip away the abstraction of high-level APIs and dive into the inner mechanics of building a neural network step-by-step using only low-level TensorFlow operations (tf.Variable, tf.matmul, tf.nn, etc.). This exercise gives you a deeper appreciation of what libraries like tf.keras automate for us\u2014and how neural networks actually operate under the hood.</p> <p>By the end of this chapter, you\u2019ll be able to:</p> <ul> <li>Initialize weights and biases manually  </li> <li>Write your own forward pass function  </li> <li>Calculate loss and accuracy  </li> <li>Implement backpropagation using tf.GradientTape  </li> <li>Train a minimal network on a real dataset (e.g., MNIST)  </li> </ul>"},{"location":"chapter14_nn_from_scratch/#step-1-dataset-preparation","title":"Step 1: Dataset Preparation","text":"<p>We\u2019ll use the MNIST dataset (handwritten digits) for simplicity. It's preloaded in TensorFlow: <pre><code>import tensorflow as tf\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize and flatten\nx_train, x_test = x_train / 255.0, x_test / 255.0\nx_train = x_train.reshape(-1, 784)\nx_test = x_test.reshape(-1, 784)\n\n# Convert to tf.Tensor\nx_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\ny_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\nx_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\ny_test = tf.convert_to_tensor(y_test, dtype=tf.int64)\n</code></pre></p>"},{"location":"chapter14_nn_from_scratch/#step-2-model-initialization","title":"Step 2: Model Initialization","text":"<p>We'll define a simple feedforward neural network with:</p> <ul> <li>Input layer: 784 units (28x28 pixels)  </li> <li>Hidden layer: 128 units + ReLU  </li> <li>Output layer: 10 units (one per digit)</li> </ul> <pre><code># Parameters\ninput_size = 784\nhidden_size = 128\noutput_size = 10\n\n# Weights and biases\nW1 = tf.Variable(tf.random.normal([input_size, hidden_size], stddev=0.1))\nb1 = tf.Variable(tf.zeros([hidden_size]))\nW2 = tf.Variable(tf.random.normal([hidden_size, output_size], stddev=0.1))\nb2 = tf.Variable(tf.zeros([output_size]))\n</code></pre>"},{"location":"chapter14_nn_from_scratch/#step-3-forward-pass-function","title":"Step 3: Forward Pass Function","text":"<pre><code>def forward_pass(x):\n    hidden = tf.nn.relu(tf.matmul(x, W1) + b1)\n    logits = tf.matmul(hidden, W2) + b2\n    return logits\n</code></pre>"},{"location":"chapter14_nn_from_scratch/#step-4-loss-accuracy","title":"Step 4: Loss &amp; Accuracy","text":"<p>Use sparse categorical cross-entropy since labels are integer-encoded: <pre><code>def compute_loss(logits, labels):\n    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n\ndef compute_accuracy(logits, labels):\n    preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n    return tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n</code></pre></p>"},{"location":"chapter14_nn_from_scratch/#step-5-training-loop","title":"Step 5: Training Loop","text":"<p>Now we manually implement the training loop using <code>tf.GradientTape</code>.</p> <pre><code>optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nepochs = 5\nbatch_size = 64\n\nfor epoch in range(epochs):\n    for i in range(0, len(x_train), batch_size):\n        x_batch = x_train[i:i+batch_size]\n        y_batch = y_train[i:i+batch_size]\n\n        with tf.GradientTape() as tape:\n            logits = forward_pass(x_batch)\n            loss = compute_loss(logits, y_batch)\n\n        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n        optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n\n    # Epoch-end evaluation\n    test_logits = forward_pass(x_test)\n    test_acc = compute_accuracy(test_logits, y_test)\n    print(f\"Epoch {epoch+1}, Test Accuracy: {test_acc:.4f}\")\n</code></pre>"},{"location":"chapter14_nn_from_scratch/#summary","title":"Summary","text":"<p>In this chapter, we:  </p> <ul> <li> <p>Built a fully functioning neural network without tf.keras  </p> </li> <li> <p>Initialized all parameters manually  </p> </li> <li> <p>Defined forward propagation, loss, and backpropagation  </p> </li> <li> <p>Trained it on MNIST using gradient descent  </p> </li> </ul> <p>Understanding how to manually construct and train a neural network builds foundational intuition that will help you:</p> <ul> <li> <p>Debug custom layers and losses  </p> </li> <li> <p>Understand performance bottlenecks  </p> </li> <li> <p>Transition into low-level model tweaking when needed</p> </li> </ul>"},{"location":"chapter15_layers/","title":"Chapter 15: Layers &amp; Activation Functions","text":"<p>\u201c*Neurons speak in activations. Layers translate</p> <p>High-level APIs in TensorFlow make it easy to build neural networks using predefined layers and activation functions. But beneath these abstractions lies the mathematical logic we explored in Chapter 14.</p> <p>This chapter dives into:</p> <ul> <li>Commonly used layers in tf.keras.layers  </li> <li>How layers are composed in Sequential and Functional APIs  </li> <li>The role and behavior of activation functions  </li> <li>How to visualize activation functions  </li> <li>Choosing the right activation based on task  </li> </ul> <p>By the end, you\u2019ll grasp how layers and activations form the \u201cbuilding blocks\u201d of deep learning architectures.</p>"},{"location":"chapter15_layers/#understanding-layers","title":"\ud83c\udfd7\ufe0f Understanding Layers","text":"<p>Layers are wrappers around mathematical functions that transform input tensors. Common types include:</p>"},{"location":"chapter15_layers/#1-dense-layer","title":"1. Dense Layer","text":"<p>Fully-connected layer where every neuron receives input from all neurons in the previous layer.</p> <pre><code>from tensorflow.keras.layers import Dense\n\ndense_layer = Dense(units=64, activation='relu')\n</code></pre>"},{"location":"chapter15_layers/#2-dropout-layer","title":"2. Dropout Layer","text":"<p>Randomly disables a fraction of units during training to prevent overfitting.</p> <pre><code>from tensorflow.keras.layers import Dropout\n\ndropout_layer = Dropout(rate=0.5)  # Drops 50% of units\n</code></pre>"},{"location":"chapter15_layers/#3-flatten-layer","title":"3. Flatten Layer","text":"<p>Converts multi-dimensional input (e.g., 28x28 image) to 1D vector.</p> <pre><code>from tensorflow.keras.layers import Flatten\n\nflatten_layer = Flatten()\n</code></pre>"},{"location":"chapter15_layers/#activation-functions","title":"\u26a1 Activation Functions","text":"<p>Activation functions determine whether a neuron \u201cfires\u201d or not. They introduce non-linearity\u2014critical for learning complex patterns.</p>"},{"location":"chapter15_layers/#common-activation-functions","title":"\ud83d\udd39 Common Activation Functions","text":"Function Formula Use Case ReLU max(0, x) Most common; avoids vanishing gradients Sigmoid 1 / (1 + e^-x) Binary classification (last layer) Tanh (e^x - e^-x) / (e^x + e^-x) Good zero-centered activation Softmax exp(x_i) / \u03a3 exp(x_j) (multi-class) Output layer for multi-class problems Leaky ReLU x if x &gt; 0 else \u03b1 * x (\u03b1 ~ 0.01) Avoids dead ReLU units"},{"location":"chapter15_layers/#visualizing-activations","title":"Visualizing Activations","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nx = tf.linspace(-10.0, 10.0, 400)\n\nactivations = {\n    \"ReLU\": tf.nn.relu(x),\n    \"Sigmoid\": tf.nn.sigmoid(x),\n    \"Tanh\": tf.nn.tanh(x),\n    \"Leaky ReLU\": tf.nn.leaky_relu(x),\n}\n\nplt.figure(figsize=(10, 6))\nfor name, y in activations.items():\n    plt.plot(x, y, label=name)\nplt.legend()\nplt.title(\"Activation Functions\")\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"chapter15_layers/#build-a-network-with-layers-activations","title":"Build a Network with Layers &amp; Activations","text":"<p>Here\u2019s how you can simplify your neural net from Chapter 14 using layers:</p> <pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\n\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),\n    Dense(128, activation='relu'),\n    Dense(10)  # No softmax here; included in loss\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nmodel.summary()\n</code></pre>"},{"location":"chapter15_layers/#functional-api-example","title":"Functional API Example","text":"<p>The Functional API allows for more complex architectures (e.g., multi-input, multi-output):</p> <pre><code>from tensorflow.keras import Model, Input\n\ninputs = Input(shape=(28, 28))\nx = Flatten()(inputs)\nx = Dense(128, activation='relu')(x)\noutputs = Dense(10)(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"chapter15_layers/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Explored key layer types (Dense, Flatten, Dropout)  </li> <li>Learned how activation functions shape neural computations  </li> <li>Built models using Sequential and Functional APIs  </li> <li>Visualized and compared activation behaviors  </li> </ul> <p>You now understand how layers and activations turn your raw tensors into meaningful representations that a neural network can learn from.</p>"},{"location":"chapter16_loss_optim/","title":"Chapter 16: Loss Functions &amp; Optimizers","text":"<p>\u201cWithout a compass, even the smartest network gets lost. Loss guides learning. Optimization moves us forward.\u201d</p> <p>In this chapter, we explore two of the most crucial ingredients of any machine learning recipe:</p> <ul> <li>Loss functions: Measure how far off our model\u2019s predictions are from the actual values.  </li> <li>Optimizers: Algorithms that adjust model parameters to minimize this loss.</li> </ul> <p>By the end, you'll understand:</p> <ul> <li>The difference between various loss functions and when to use them  </li> <li>How gradients are computed and used  </li> <li>Popular optimization algorithms and their trade-offs  </li> <li>How to implement custom loss functions and plug them into training</li> </ul>"},{"location":"chapter16_loss_optim/#what-is-a-loss-function","title":"What Is a Loss Function?","text":"<p>A loss function tells us how \u201cbad\u201d our predictions are. It is scalar-valued, allowing TensorFlow to compute gradients via backpropagation.</p> <p>\ud83d\udd39 Common Losses in TensorFlow</p> Task Loss Function TensorFlow API Binary classification Binary Crossentropy tf.keras.losses.BinaryCrossentropy() Multi-class classification Sparse Categorical Crossentropy tf.keras.losses.SparseCategoricalCrossentropy() Regression (real values) Mean Squared Error tf.keras.losses.MeanSquaredError()"},{"location":"chapter16_loss_optim/#example-sparse-categorical-crossentropy","title":"Example: Sparse Categorical Crossentropy","text":"<p><pre><code>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss = loss_fn(y_true, y_pred)\n</code></pre> - <code>from_logits=True</code> means the model outputs raw values (logits) without softmax.</p> <ul> <li>If your model outputs softmax-activated values, set f<code>rom_logits=False</code>.</li> </ul>"},{"location":"chapter16_loss_optim/#what-are-optimizers","title":"What Are Optimizers?","text":"<p>Optimizers update model parameters using gradients computed from the loss. They are essential for gradient descent-based training.</p> <p>\ud83d\udd39 Popular Optimizers</p> Optimizer Description Usage SGD Stochastic Gradient Descent SGD(learning_rate=0.01) Momentum Adds inertia to SGD SGD(momentum=0.9) RMSProp Adjusts learning rate based on recent magnitudes RMSprop(learning_rate=0.001) Adam Combines Momentum + RMSProp Adam(learning_rate=0.001)"},{"location":"chapter16_loss_optim/#example-compile-with-optimizer","title":"Example: Compile with Optimizer","text":"<pre><code>model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n</code></pre>"},{"location":"chapter16_loss_optim/#custom-loss-function","title":"Custom Loss Function","text":"<p>Sometimes, built-in loss functions aren\u2019t enough. Here\u2019s how you can define your own:</p> <pre><code>def custom_mse_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred))\n</code></pre> <p>Plug into model like this:</p> <pre><code>model.compile(\n    optimizer='adam',\n    loss=custom_mse_loss\n)\n</code></pre>"},{"location":"chapter16_loss_optim/#custom-training-loop-optional-recap","title":"Custom Training Loop (Optional Recap)","text":"<p>When not using model.fit(), you need to compute loss and apply gradients manually:</p> <p><pre><code>with tf.GradientTape() as tape:\n    logits = model(x_batch, training=True)\n    loss_value = loss_fn(y_batch, logits)\n\ngrads = tape.gradient(loss_value, model.trainable_variables)\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n</code></pre> This gives you full control over training and is often used in research or advanced custom workflows.</p>"},{"location":"chapter16_loss_optim/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Loss functions quantify how wrong a model\u2019s predictions are.  </li> <li>Optimizers use gradients to update model weights and minimize loss.  </li> <li>Adam is a great default optimizer, but others may work better depending on the problem.  </li> <li>You can define custom loss functions for flexibility.  </li> </ul> <p>Understanding the relationship between loss \u2192 gradient \u2192 optimizer \u2192 new weights is the key to mastering how neural networks learn.</p>"},{"location":"chapter17_backprop/","title":"Chapter 17: Backpropagation &amp; Gradient Descent","text":"<p>\u201cA neural network learns by looking backward\u2014adjusting the past to improve the future.\u201d</p> <p>This chapter reveals the mechanism that powers all deep learning: backpropagation.</p> <p>We will:</p> <ul> <li>Understand how gradients are computed using the chain rule  </li> <li>Visualize how errors flow backward in a network  </li> <li>See how gradient descent uses those gradients to update weights  </li> <li>Use tf.GradientTape to track gradients  </li> <li>Step through a simplified manual backpropagation demo</li> </ul>"},{"location":"chapter17_backprop/#the-core-idea-chain-rule","title":"The Core Idea: Chain Rule","text":"<p>Backpropagation uses the chain rule from calculus to compute the gradient of the loss function with respect to each parameter in the network.</p> <p>In a simple neural net:</p> <pre><code>Input x \u2192 [W1, b1] \u2192 hidden \u2192 [W2, b2] \u2192 output \u2192 loss\n</code></pre> <p>We want to know:</p> <p><pre><code>\u2202Loss/\u2202W1, \u2202Loss/\u2202b1, \u2202Loss/\u2202W2, \u2202Loss/\u2202b2\n</code></pre> TensorFlow does this using automatic differentiation.</p>"},{"location":"chapter17_backprop/#tfgradienttape-tensorflows-engine","title":"tf.GradientTape: TensorFlow\u2019s Engine","text":"<p><pre><code>with tf.GradientTape() as tape:\n    logits = model(x_batch)\n    loss = loss_fn(y_batch, logits)\n\ngradients = tape.gradient(loss, model.trainable_variables)\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\n</code></pre> - GradientTape watches all operations to record them. - When tape.gradient() is called, TensorFlow traces those operations backward using the chain rule.</p>"},{"location":"chapter17_backprop/#manual-backpropagation-a-tiny-example","title":"Manual Backpropagation: A Tiny Example","text":"<p>Let\u2019s build a single-layer model manually and compute gradients ourselves.</p> <ol> <li>Define Inputs and Parameters</li> </ol> <pre><code>x = tf.constant([[1.0, 2.0]])\ny_true = tf.constant([[1.0]])\n\nW = tf.Variable([[0.1], [0.2]])\nb = tf.Variable([0.3])\n</code></pre> <ol> <li>Forward Pass and Loss</li> </ol> <pre><code>def forward(x):\n    return tf.matmul(x, W) + b\n\ndef mse(y_pred, y_true):\n    return tf.reduce_mean(tf.square(y_pred - y_true))\n</code></pre> <ol> <li>Compute Gradients</li> </ol> <pre><code>with tf.GradientTape() as tape:\n    y_pred = forward(x)\n    loss = mse(y_pred, y_true)\n\ngrads = tape.gradient(loss, [W, b])\n</code></pre> <ol> <li>Apply Gradients</li> </ol> <p><pre><code>optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\noptimizer.apply_gradients(zip(grads, [W, b]))\n</code></pre> This is manual backpropagation with a single-layer network. The same process scales up to thousands of layers internally!</p>"},{"location":"chapter17_backprop/#gradient-descent-the-learning-step","title":"Gradient Descent: The Learning Step","text":"<p>At every training step, gradient descent does this:</p> <p><pre><code>new_weight = old_weight - learning_rate * gradient\n</code></pre> Variants like Adam, RMSProp, etc., optimize this update rule by adapting learning rates.</p>"},{"location":"chapter17_backprop/#intuition-why-does-this-work","title":"Intuition: Why Does This Work?","text":"<p>Imagine trying to descend a mountain blindfolded, feeling the slope with your feet. Gradient descent gives you the direction (steepest descent) and a step size. Backpropagation tells you how each step affects your overall position (loss).</p> <p>Together, they let the network learn even in high-dimensional, abstract spaces.</p>"},{"location":"chapter17_backprop/#summary","title":"Summary","text":"<p>In this chapter, we:  </p> <ul> <li>Demystified backpropagation using the chain rule  </li> <li>Used <code>tf.GradientTape</code> to compute gradients automatically  </li> <li>Performed a step-by-step manual backpropagation  </li> <li>Understood how gradient descent updates weights toward lower loss</li> </ul> <p>Backpropagation isn\u2019t just a technique\u2014it\u2019s the soul of deep learning. Mastering it gives you power to customize and debug any neural architecture.</p>"},{"location":"chapter18_training/","title":"Chapter 18: Model Training with fit() and evaluate()","text":"<p>\u201cWith great abstraction comes great productivity. <code>fit()</code> lets you train deep models with a single line.\u201d</p> <p>After understanding the fundamentals of backpropagation and gradient descent, you now know what goes on under the hood. In this chapter, we shift our focus to using TensorFlow\u2019s high-level training interface\u2014the <code>fit()</code> and <code>evaluate()</code> methods from the <code>tf.keras.Model</code> class.</p> <p>By the end, you\u2019ll be able to:</p> <ul> <li>Train models with <code>model.fit()</code> using real datasets  </li> <li>Track performance with <code>model.evaluate()</code> and <code>model.predict()</code> </li> <li>Customize training with validation splits, callbacks, and batch sizes  </li> <li>Monitor overfitting and training speed</li> </ul>"},{"location":"chapter18_training/#setup-a-simple-model","title":"Setup: A Simple Model","text":"<p>Let\u2019s use the MNIST dataset and a fully connected neural net.</p> <pre><code>import tensorflow as tf\n\n# Load &amp; preprocess data\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Define model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10)\n])\n</code></pre>"},{"location":"chapter18_training/#training-with-fit","title":"Training with fit()","text":"<p><pre><code>model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nmodel.fit(\n    x_train, y_train,\n    epochs=5,\n    batch_size=64,\n    validation_split=0.2\n)\n</code></pre> - epochs: number of full passes through the training data - batch_size: how many samples per gradient update - validation_split: % of training data used for validation</p>"},{"location":"chapter18_training/#evaluation-and-prediction","title":"Evaluation and Prediction","text":"<p>Once trained, evaluate the model:</p> <pre><code>test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n</code></pre> <p>Make predictions:</p> <pre><code>logits = model.predict(x_test)\npredictions = tf.argmax(logits, axis=1)\n</code></pre>"},{"location":"chapter18_training/#using-callbacks","title":"Using Callbacks","text":"<p>Callbacks allow you to hook into the training process. Common uses:</p> <ul> <li>Early stopping  </li> <li>Model checkpointing  </li> <li>Logging</li> </ul> <pre><code>callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\nmodel.fit(\n    x_train, y_train,\n    epochs=20,\n    validation_split=0.2,\n    callbacks=[callback]\n)\n</code></pre>"},{"location":"chapter18_training/#custom-batching-with-tfdata","title":"Custom Batching with <code>tf.data</code>","text":"<p>For more control, use <code>tf.data.Dataset</code>: <pre><code>train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_ds = train_ds.shuffle(buffer_size=1024).batch(64)\n\nmodel.fit(train_ds, epochs=5)\n</code></pre> This is especially useful for large datasets, streaming data, or augmentations.</p>"},{"location":"chapter18_training/#monitoring-training","title":"Monitoring Training","text":"<p>To track progress visually, use the <code>history</code> object: <pre><code>history = model.fit(...)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.legend()\n</code></pre></p>"},{"location":"chapter18_training/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>How <code>fit()</code> abstracts the entire training loop  </li> <li>How to validate, evaluate, and predict using simple APIs  </li> <li>How callbacks improve training flexibility  </li> <li>How to use <code>tf.data</code> for custom batching</li> </ul> <p>The <code>fit()</code> interface doesn\u2019t replace your understanding of backprop\u2014it supercharges it. Now you can focus on what to train, not just how to train it.</p>"},{"location":"chapter19_saving_loading/","title":"Chapter 19: Saving, Loading, and Callbacks","text":"<p>\u201cTraining is hard-earned. Never lose progress. Save smart, load fast, and train with foresight.\u201d</p> <p>This chapter focuses on one of the most practical and essential aspects of any machine learning workflow: saving and restoring models and automating training behavior with callbacks.</p> <p>By the end, you'll know how to:  </p> <ul> <li>Save your model weights or full architecture  </li> <li>Load and resume training or inference  </li> <li>Use callbacks like EarlyStopping, ModelCheckpoint, and custom logic  </li> <li>Ensure your work survives interruptions and scales reliably</li> </ul>"},{"location":"chapter19_saving_loading/#saving-your-model","title":"Saving Your Model","text":"<p>There are two main formats in TensorFlow:</p>"},{"location":"chapter19_saving_loading/#1-savedmodel-format-recommended","title":"1. SavedModel Format (Recommended)","text":"<p><pre><code>model.save('saved_model/my_model')\n</code></pre> To load: <pre><code>new_model = tf.keras.models.load_model('saved_model/my_model')\n</code></pre> \u2705 Includes:</p> <ul> <li>Architecture  </li> <li>Weights  </li> <li>Training config  </li> <li>Optimizer state  </li> </ul>"},{"location":"chapter19_saving_loading/#2-hdf5-format-h5","title":"2. HDF5 Format (.h5)","text":"<p><pre><code>model.save('my_model.h5')\nnew_model = tf.keras.models.load_model('my_model.h5')\n</code></pre> This format is widely used in other frameworks but has limitations with newer custom layers and optimizers.</p>"},{"location":"chapter19_saving_loading/#saving-only-weights","title":"Saving Only Weights","text":"<p>If you just want to save parameters: <pre><code>model.save_weights('weights.ckpt')\n</code></pre> To load them into the same architecture: <pre><code>model.load_weights('weights.ckpt')\n</code></pre> Useful when:</p> <ul> <li>You want to share weights but not the model code  </li> <li>You're using subclassed models</li> </ul>"},{"location":"chapter19_saving_loading/#using-callbacks","title":"Using Callbacks","text":"<p>Callbacks are hooks into the training loop that let you intervene at key moments: epoch end, batch end, etc.</p>"},{"location":"chapter19_saving_loading/#common-callbacks","title":"\ud83d\udd39 Common Callbacks","text":"<p>1. Early Stopping Stop training if validation loss doesn\u2019t improve. <pre><code>tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n</code></pre> 2. Model Checkpoint Save model during training. <pre><code>tf.keras.callbacks.ModelCheckpoint(\n    filepath='best_model.h5',\n    save_best_only=True,\n    monitor='val_loss',\n    mode='min'\n)\n</code></pre> 3. Learning Rate Scheduler Adjust learning rate during training. <pre><code>tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 0.95 ** epoch)\n</code></pre></p>"},{"location":"chapter19_saving_loading/#using-callbacks-in-fit","title":"Using Callbacks in <code>fit()</code>","text":"<pre><code>callbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n    tf.keras.callbacks.ModelCheckpoint(filepath='best_model.keras', save_best_only=True)\n]\n\nmodel.fit(\n    x_train, y_train,\n    validation_split=0.2,\n    epochs=20,\n    callbacks=callbacks\n)\n</code></pre>"},{"location":"chapter19_saving_loading/#custom-callback-advanced","title":"Custom Callback (Advanced)","text":"<p>You can define your own logic: <pre><code>class PrintEpoch(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"Epoch {epoch+1} finished. Accuracy: {logs['accuracy']:.4f}\")\n</code></pre></p> <p>Use with: <pre><code>model.fit(..., callbacks=[PrintEpoch()])\n</code></pre></p>"},{"location":"chapter19_saving_loading/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Learned to save and load models in multiple formats  </li> <li>Used callbacks to control training and automate decisions  </li> <li>Understood when to save weights only vs full model  </li> <li>Built your own custom callback for fine-grained control</li> </ul> <p>These tools are critical for long training runs, collaborative projects, and production deployment. Always train as if your power might go out. \u26a1</p>"},{"location":"chapter1_what_is_tensorflow/","title":"TensorFlow Builder's Companion Book","text":"<p>Welcome to the official beginning of your TensorFlow journey\u2014 A companion for builders, tinkerers, and deep learning dreamers.</p>"},{"location":"chapter1_what_is_tensorflow/#chapter-1-what-is-tensorflow","title":"Chapter 1: What is TensorFlow?","text":"<p>\u201cWith tensors as our bricks and computation graphs as our scaffold, we build minds from math.\u201d</p> <p>Imagine a massive data factory. At one end, raw data enters. At the other, intelligent predictions come out. In between? Conveyor belts, robotic arms, quality control systems\u2014constantly crunching numbers, transforming data, and learning over time.</p> <p>That\u2019s TensorFlow.</p> <p>TensorFlow is an end-to-end open-source platform for machine learning, originally developed by the Google Brain team. It enables developers to:  </p> <ul> <li> <p>Build and train neural networks</p> </li> <li> <p>Perform automatic differentiation</p> </li> <li> <p>Run models on CPUs, GPUs, TPUs, mobile devices, browsers, or servers</p> </li> <li> <p>Scale easily from research notebooks to production-ready pipelines</p> </li> </ul> <p>It\u2019s not just a library\u2014it\u2019s an ecosystem.</p>"},{"location":"chapter1_what_is_tensorflow/#11-core-features-at-a-glance","title":"1.1 Core Features at a Glance","text":"<ul> <li> <p><code>tf.Tensor</code>: the atomic unit of computation\u2014just like PyTorch's <code>torch.Tensor</code></p> </li> <li> <p><code>tf.Variable</code>: trainable tensors for model weights</p> </li> <li> <p><code>tf.GradientTape</code>: automatic differentiation engine</p> </li> <li> <p><code>tf.data</code>: scalable input pipelines</p> </li> <li> <p><code>tf.function</code>: compile Python into TensorFlow graph</p> </li> <li> <p><code>tf.keras</code>: high-level API to define &amp; train models</p> </li> <li> <p><code>TFLite</code>, <code>TFX</code>, <code>TF-Serving</code>: deploy models to mobile, pipelines, or production</p> </li> </ul>"},{"location":"chapter1_what_is_tensorflow/#12-quick-test-check-tensorflow-installation-gpu","title":"1.2 Quick Test: Check TensorFlow Installation &amp; GPU","text":"<p><pre><code>import tensorflow as tf\n\ndef print_gpu_info():\n    print(\"TensorFlow version:\", tf.__version__)\n    gpus = tf.config.list_physical_devices('GPU')\n    print(\"Num GPUs Available:\", len(gpus))\n    for gpu in gpus:\n        print(\"GPU Detected:\", gpu.name)\n\nif __name__ == '__main__':\n    print_gpu_info()\n</code></pre> Save it as <code>check_tf_gpu.py</code> and run it with:</p> <p><pre><code>python check_tf_gpu.py\n</code></pre> You should see your GPU model (ex. NVIDIA RTX 4050 listed). If yes\u2014welcome to GPU-accelerated machine learning. </p>"},{"location":"chapter1_what_is_tensorflow/#13-why-tensorflow-vs-others","title":"1.3 Why TensorFlow (vs others)?","text":"Feature TensorFlow PyTorch Graph Execution @tf.function (static graph) Eager by default Mobile Deployment TensorFlow Lite, TFLM PyTorch Mobile (early stage) Production Pipelines TensorFlow Extended (TFX) TorchServe, FastAPI Debugging Harder due to graphs Easier due to eager mode Community Huge (Google-backed) Huge (Meta-backed) <p>In short: if you care about mobile deployment, large-scale production, or research+product fusion, TensorFlow is worth the climb.</p>"},{"location":"chapter20_visualizing/","title":"Chapter 20: Visualizing Model Progress with TensorBoard","text":"<p>\u201cWhat you can\u2019t see, you can\u2019t debug. TensorBoard turns numbers into insight.\u201d</p> <p>Training deep learning models involves a flurry of metrics, graphs, losses, weights, and gradients. TensorBoard is TensorFlow\u2019s built-in visualization toolkit\u2014your window into the learning process.</p> <p>In this chapter, you\u2019ll:</p> <ul> <li>Learn how to launch TensorBoard  </li> <li>Track training metrics, histograms, and model graphs  </li> <li>Visualize embeddings and profiler data  </li> <li>Log custom scalars, images, and text during training  </li> <li>Debug and tune your models more efficiently</li> </ul>"},{"location":"chapter20_visualizing/#what-is-tensorboard","title":"What Is TensorBoard?","text":"<p>TensorBoard is a web-based dashboard that visualizes:</p> <ul> <li>Loss and accuracy curves  </li> <li>Weight and bias distributions  </li> <li>Learning rates and gradients  </li> <li>Computation graphs  </li> <li>Embeddings in 2D/3D  </li> <li>Profiling info (CPU/GPU usage)</li> </ul> <p>It helps: \u2705 Debug your training \u2705 Monitor model convergence \u2705 Tune hyperparameters \u2705 Share results with others</p>"},{"location":"chapter20_visualizing/#step-1-setup-a-logging-directory","title":"Step 1: Setup a Logging Directory","text":"<pre><code>import datetime\n\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n</code></pre> <p>Attach it to <code>fit()</code>: <pre><code>model.fit(\n    x_train, y_train,\n    epochs=5,\n    validation_split=0.2,\n    callbacks=[tensorboard_cb]\n)\n</code></pre></p>"},{"location":"chapter20_visualizing/#step-2-launch-tensorboard","title":"Step 2: Launch TensorBoard","text":"<p>In terminal: <pre><code>tensorboard --logdir logs/fit\n</code></pre></p> <p>Or in notebooks: <pre><code>%load_ext tensorboard\n%tensorboard --logdir logs/fit\n</code></pre> Visit: <code>http://localhost:6006</code> (Or open the hosted link in Colab)</p>"},{"location":"chapter20_visualizing/#what-can-you-visualize","title":"What Can You Visualize?","text":""},{"location":"chapter20_visualizing/#scalars","title":"\ud83d\udd39 Scalars","text":"<p>Loss, accuracy, learning rate, etc.</p>"},{"location":"chapter20_visualizing/#histograms","title":"\ud83d\udd39 Histograms","text":"<p>Distributions of weights, biases, activations.</p>"},{"location":"chapter20_visualizing/#graph","title":"\ud83d\udd39 Graph","text":"<p>Model computation graph for debugging layers and shapes.</p>"},{"location":"chapter20_visualizing/#embeddings","title":"\ud83d\udd39 Embeddings","text":"<p>Visualize high-dimensional features (like word embeddings) using t-SNE or PCA.</p>"},{"location":"chapter20_visualizing/#images","title":"\ud83d\udd39 Images","text":"<p>Track model outputs visually, e.g., generated images.</p> <pre><code>file_writer = tf.summary.create_file_writer(log_dir + \"/images\")\n\nwith file_writer.as_default():\n    tf.summary.image(\"Sample images\", sample_batch, step=0)\n</code></pre>"},{"location":"chapter20_visualizing/#profile-model-performance","title":"Profile Model Performance","text":"<p>You can enable profiling to analyze slowdowns: <pre><code>tensorboard_cb = tf.keras.callbacks.TensorBoard(\n    log_dir=log_dir,\n    histogram_freq=1,\n    profile_batch='500,520'  # Profile between these batches\n)\n</code></pre> This provides:</p> <ul> <li>CPU/GPU usage  </li> <li>Memory footprint  </li> <li>Bottlenecks in data loading or model ops  </li> </ul>"},{"location":"chapter20_visualizing/#custom-logging-advanced","title":"Custom Logging (Advanced)","text":"<p>Log custom scalars or text: <pre><code>with file_writer.as_default():\n    tf.summary.scalar('custom_metric', value, step=epoch)\n    tf.summary.text('note', 'Validation dropped sharply!', step=epoch)\n</code></pre></p>"},{"location":"chapter20_visualizing/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Set up TensorBoard to monitor training  </li> <li>Explored scalar, histogram, and graph visualizations  </li> <li>Learned to profile and debug performance  </li> <li>Logged custom metrics for advanced diagnostics</li> </ul> <p>TensorBoard transforms your training sessions into stories, making every epoch readable, trackable, and shareable. It\u2019s a must-have for experimentation and research.</p>"},{"location":"chapter21_text_tokenization/","title":"Chapter 21: Text Preprocessing &amp; Tokenization","text":"<p>\u201cIn the world of language, meaning is hidden in tokens. Before a model can read, we must teach it to parse.\u201d</p> <p>Natural Language Processing (NLP) starts with text, but machines only understand numbers. This chapter introduces the crucial preprocessing steps needed to convert raw language into model-ready tensors.</p> <p>You\u2019ll learn:</p> <ul> <li>Why text preprocessing is necessary  </li> <li>How tokenization works in TensorFlow  </li> <li>How to handle sequences of different lengths  </li> <li>The role of padding, truncation, and vocab management  </li> <li>How to build a Tokenizer pipeline using tf.keras and TextVectorization</li> </ul> <p>By the end, you\u2019ll be ready to feed text into deep learning models.</p>"},{"location":"chapter21_text_tokenization/#why-preprocessing-matters","title":"Why Preprocessing Matters","text":"<ul> <li>Text is messy:  </li> <li>Varying lengths  </li> <li>Punctuation, emojis, casing  </li> <li>Slang, typos, multiple languages</li> </ul> <p>We must normalize and vectorize text into a format that TensorFlow can understand.</p>"},{"location":"chapter21_text_tokenization/#step-1-text-normalization","title":"Step 1: Text Normalization","text":"<p>Standard steps include:</p> <ul> <li>Lowercasing  </li> <li>Removing punctuation  </li> <li>Handling contractions or special characters</li> </ul> <pre><code>import re\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\n</code></pre> <p>But TensorFlow offers a cleaner way via <code>TextVectorization</code>.</p>"},{"location":"chapter21_text_tokenization/#step-2-tokenization-with-textvectorization","title":"Step 2: Tokenization with <code>TextVectorization</code>","text":"<p><pre><code>from tensorflow.keras.layers import TextVectorization\n\n# Example data\ntexts = tf.constant([\"TensorFlow is great!\", \"Natural Language Processing.\"])\n\n# Create vectorizer\nvectorizer = TextVectorization(\n    max_tokens=1000,\n    output_mode='int',\n    output_sequence_length=6\n)\n\nvectorizer.adapt(texts)\nvectorized = vectorizer(texts)\nprint(vectorized)\n</code></pre> Output: Each sentence is converted into a fixed-length sequence of integers (tokens).</p>"},{"location":"chapter21_text_tokenization/#sequence-padding","title":"Sequence Padding","text":"<p>Neural networks (like RNNs or Transformers) require fixed-length inputs.</p> <p>If using <code>Tokenizer</code> + <code>pad_sequences</code> manually: <pre><code>from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=1000, oov_token=\"&lt;OOV&gt;\")\ntokenizer.fit_on_texts(texts)\n\nsequences = tokenizer.texts_to_sequences(texts)\npadded = pad_sequences(sequences, maxlen=6, padding='post')\n</code></pre></p>"},{"location":"chapter21_text_tokenization/#padding-options","title":"Padding Options","text":"Argument Options Description padding 'pre', 'post' Add zeros before or after sequence truncating 'pre', 'post' Cut long sequences before or after maxlen int Final sequence length after padding"},{"location":"chapter21_text_tokenization/#view-token-to-word-mapping","title":"View Token-to-Word Mapping","text":"<pre><code>word_index = tokenizer.word_index\nprint(word_index['tensorflow'])  # Shows token assigned to word\n</code></pre>"},{"location":"chapter21_text_tokenization/#dealing-with-out-of-vocab-oov","title":"Dealing with Out-of-Vocab (OOV)","text":"<p>Words not seen during f<code>it_on_texts()</code> are mapped to a special token (e.g., <code>&lt;OOV&gt;</code>).</p> <p>Ensure you define <code>oov_token='&lt;OOV&gt;'</code> to handle unseen text gracefully during inference.</p>"},{"location":"chapter21_text_tokenization/#word-embeddings-preview","title":"Word Embeddings (Preview)","text":"<p>Once tokenized, you can pass sequences into an Embedding layer: <pre><code>from tensorflow.keras.layers import Embedding\n\nembedding = Embedding(input_dim=1000, output_dim=16, input_length=6)\noutput = embedding(vectorized)\n</code></pre> This converts token indices into dense, learnable vector representations.</p>"},{"location":"chapter21_text_tokenization/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Learned to clean and normalize raw text  </li> <li>Used TextVectorization for modern tokenization  </li> <li>Managed sequence length via padding and truncation  </li> <li>Prepared inputs for embeddings and models</li> </ul> <p>Text preprocessing is the foundation of any NLP pipeline. With clean, consistent input, models can focus on learning patterns\u2014not battling messy data.</p>"},{"location":"chapter22_TF_IDF/","title":"Chapter 22: TF-IDF, Bag-of-Words Representations","text":"<p>\u201cBefore transformers, we counted words. And in that counting, meaning emerged.\u201d</p> <p>Before deep learning, NLP was powered by statistical representations of text\u2014primarily Bag-of-Words (BoW) and TF-IDF.</p> <p>These techniques are still useful today:</p> <ul> <li>They\u2019re fast, interpretable, and effective on small datasets  </li> <li>Often used as baselines before training heavy models  </li> <li>Key to understanding how NLP evolved into embedding-based approaches</li> </ul> <p>In this chapter, you'll learn:</p> <ul> <li>How to convert text into BoW and TF-IDF vectors  </li> <li>When to use count-based vs frequency-based methods  </li> <li>How to implement these using <code>TfidfVectorizer</code> and <code>CountVectorizer</code> from <code>scikit-learn</code> </li> <li>How to visualize feature importance</li> </ul>"},{"location":"chapter22_TF_IDF/#bag-of-words-bow","title":"Bag-of-Words (BoW)","text":"<p>BoW represents text by counting how often each word appears, ignoring order.</p> <p>Example: <pre><code>Doc 1: \"TensorFlow is great\"\nDoc 2: \"TensorFlow and Keras\"\n\nVocabulary: ['and', 'great', 'is', 'keras', 'tensorflow']\n\nVectors:\nDoc 1 \u2192 [0, 1, 1, 0, 1]\nDoc 2 \u2192 [1, 0, 0, 1, 1]\n</code></pre></p>"},{"location":"chapter22_TF_IDF/#using-countvectorizer","title":"Using CountVectorizer","text":"<pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\ntexts = [\"TensorFlow is great\", \"TensorFlow and Keras\"]\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(texts)\n\nprint(vectorizer.get_feature_names_out())\nprint(X.toarray())\n</code></pre>"},{"location":"chapter22_TF_IDF/#tf-idf-term-frequencyinverse-document-frequency","title":"TF-IDF: Term Frequency\u2013Inverse Document Frequency","text":"<p>TF-IDF downweights common words (like \"is\", \"the\") and upweights rare but important ones.</p> <p>Formula:  </p> <ul> <li>TF: Number of times a word appears in a document  </li> <li>IDF: Inverse of how many documents contain that word</li> </ul> <p><pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(texts)\n\nprint(vectorizer.get_feature_names_out())\nprint(X.toarray())\n</code></pre> Each number now reflects importance, not just frequency.</p>"},{"location":"chapter22_TF_IDF/#when-to-use-what","title":"When to Use What?","text":"Use Case Technique Small text datasets TF-IDF or BoW Quick baseline for classification TF-IDF Sparse features, interpretable BoW Neural network input Embedding"},{"location":"chapter22_TF_IDF/#optional-visualize-tf-idf-features","title":"Optional: Visualize TF-IDF Features","text":"<p><pre><code>import pandas as pd\n\ndf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\nprint(df)\n</code></pre> Use this to:</p> <ul> <li>Identify key words driving predictions  </li> <li>Visualize feature distributions</li> </ul>"},{"location":"chapter22_TF_IDF/#limitations","title":"Limitations","text":"<ul> <li>Doesn\u2019t consider word order (\"not good\" vs \"good\")  </li> <li>Vocabulary must be fixed  </li> <li>Large vocab = sparse vectors  </li> <li>Not suitable for capturing contextual meaning</li> </ul> <p>That\u2019s where deep learning models (like embeddings, RNNs, Transformers) step in.</p>"},{"location":"chapter22_TF_IDF/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Learned the intuition behind BoW and TF-IDF  </li> <li>Used CountVectorizer and TfidfVectorizer in scikit-learn  </li> <li>Compared their strengths and limitations  </li> <li>Set the stage for embedding-based and deep NLP models</li> </ul> <p>Classic vectorization methods are not obsolete\u2014they\u2019re foundational, fast, and still relevant in pipelines and search systems.</p>"},{"location":"chapter23_RNN_LSTM/","title":"Chapter 23: RNNs &amp; LSTMs","text":"<p>\u201cWords are not isolated\u2014they remember what came before. RNNs give models a sense of time.\u201d</p> <p>Words in a sentence form a sequence\u2014and meaning often depends on order. Traditional models like Bag-of-Words or TF-IDF treat words as independent, which limits their ability to capture structure or context.</p> <p>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) units were among the first neural architectures designed to remember context over time\u2014a fundamental shift in how machines processed text.</p> <p>By the end of this chapter, you\u2019ll:</p> <ul> <li>Understand how RNNs and LSTMs work  </li> <li>Use tf.keras.layers.SimpleRNN and LSTM  </li> <li>Train a model on sequential data (e.g., text sentiment)  </li> <li>Visualize how memory affects predictions</li> </ul>"},{"location":"chapter23_RNN_LSTM/#what-is-an-rnn","title":"What Is an RNN?","text":"<p>RNNs process sequences one element at a time, passing hidden state from one time step to the next. <pre><code>Input:      I \u2192 love \u2192 TensorFlow\nHidden:    h0 \u2192 h1 \u2192 h2\nOutput:     y0 \u2192 y1 \u2192 y2\n</code></pre> Each output depends not just on the current input, but also on past context.</p>"},{"location":"chapter23_RNN_LSTM/#the-problem-vanishing-gradients","title":"The Problem: Vanishing Gradients","text":"<p>RNNs struggle with long-term dependencies\u2014earlier words lose influence as the sequence grows. That\u2019s where LSTMs come in.</p>"},{"location":"chapter23_RNN_LSTM/#lstms-memory-with-gates","title":"LSTMs: Memory with Gates","text":"<p>LSTMs introduce internal cell states and gates (forget, input, output) to regulate information flow.</p> <ul> <li>Forget Gate: What to forget from the previous cell  </li> <li>Input Gate: What new information to store  </li> <li>Output Gate: What to pass to the next step  </li> <li>This design helps retain useful context across longer sequences.</li> </ul>"},{"location":"chapter23_RNN_LSTM/#implementing-an-lstm-in-tensorflow","title":"Implementing an LSTM in TensorFlow","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential([\n    Embedding(input_dim=10000, output_dim=64),\n    LSTM(128),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n</code></pre>"},{"location":"chapter23_RNN_LSTM/#dataset-imdb-sentiment-binary","title":"Dataset: IMDB Sentiment (Binary)","text":"<pre><code>(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200)\n</code></pre> <p>Train the Model <pre><code>model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n</code></pre></p>"},{"location":"chapter23_RNN_LSTM/#visualizing-memory","title":"Visualizing Memory","text":"<p>Use the LSTM output layer to view how sentiment builds over time. You can create attention overlays or extract intermediate states: <pre><code>intermediate_model = tf.keras.Model(inputs=model.input, outputs=model.layers[1].output)\nlstm_output = intermediate_model.predict(x_test[:1])\n</code></pre></p>"},{"location":"chapter23_RNN_LSTM/#when-to-use-rnns-lstms","title":"When to Use RNNs / LSTMs","text":"Use Case Recommended Model Short sequences RNN Long-range memory LSTM or GRU Streaming data RNN/LSTM Parallelization needed Transformer (Next Chapter)"},{"location":"chapter23_RNN_LSTM/#gru-simpler-alternative","title":"GRU: Simpler Alternative","text":"<p>Gated Recurrent Unit (GRU) is a simplified version of LSTM:</p> <ul> <li>Combines forget and input gates  </li> <li>Fewer parameters, faster to train</li> </ul> <pre><code>from tensorflow.keras.layers import GRU\nmodel = Sequential([Embedding(10000, 64), GRU(128), Dense(1, activation='sigmoid')])\n</code></pre>"},{"location":"chapter23_RNN_LSTM/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Learned how RNNs and LSTMs retain sequential memory  </li> <li>Implemented an LSTM for text sentiment classification  </li> <li>Understood their advantages and limitations  </li> <li>Explored GRU as a lightweight alternative</li> </ul> <p>RNNs and LSTMs were the backbone of NLP before Transformers. They taught us that order and memory matter in language.</p>"},{"location":"chapter24_transformers/","title":"Chapter 24: Transformers in TensorFlow (from Scratch)","text":"<p>\u201cForget recurrence. Attention is all you need.\u201d</p> <p>Transformers marked a revolution in deep learning, replacing RNNs and LSTMs as the new foundation of sequence modeling. Their secret? Self-attention\u2014a mechanism that lets models focus on relevant parts of input, regardless of position.</p> <p>In this chapter, you will:</p> <ul> <li>Understand the intuition behind attention and transformers  </li> <li>Explore how positional encoding enables order-awareness  </li> <li>Build a simplified transformer encoder block using TensorFlow  </li> <li>See how models like BERT and GPT are built on this architecture</li> </ul>"},{"location":"chapter24_transformers/#why-transformers","title":"Why Transformers?","text":"<p>RNNs process sequences sequentially, limiting parallelism. Transformers process entire sequences in parallel, and use attention to model relationships between tokens.</p> <p>\ud83d\udd01 From RNN to Transformer <pre><code>RNN: word\u2081 \u2192 word\u2082 \u2192 word\u2083 \u2192 ...\nTransformer: [word\u2081, word\u2082, word\u2083] all at once\n</code></pre></p>"},{"location":"chapter24_transformers/#self-attention-explained","title":"Self-Attention Explained","text":"<p>Self-attention allows the model to weigh the importance of different words when encoding a particular token.</p> <p>\u201cThe bank was crowded.\u201d Self-attention helps distinguish: money bank vs river bank using context like \u201ccrowded.\u201d</p>"},{"location":"chapter24_transformers/#core-transformer-components","title":"Core Transformer Components","text":"Component Purpose Embedding Layer Convert tokens to dense vectors Positional Encoding Add order information Multi-Head Attention Attend to different parts of the sequence Feed-Forward Network Process attended representations Residual &amp; LayerNorm Improve stability and gradient flow"},{"location":"chapter24_transformers/#step-by-step-mini-transformer-encoder","title":"Step-by-Step: Mini Transformer Encoder","text":"<p>Step 1: Positional Encoding <pre><code>import numpy as np\nimport tensorflow as tf\n\ndef positional_encoding(seq_len, d_model):\n    pos = np.arange(seq_len)[:, np.newaxis]\n    i = np.arange(d_model)[np.newaxis, :]\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    angle_rads = pos * angle_rates\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    return tf.cast(angle_rads, dtype=tf.float32)\n</code></pre></p> <p>Step 2: Scaled Dot-Product Attention <pre><code>def scaled_dot_product_attention(q, k, v, mask=None):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled = matmul_qk / tf.math.sqrt(d_k)\n\n    if mask is not None:\n        scaled += (mask * -1e9)\n\n    weights = tf.nn.softmax(scaled, axis=-1)\n    return tf.matmul(weights, v)\n</code></pre></p> <p>Step 3: Multi-Head Attention Layer <pre><code>from tensorflow.keras.layers import Dense, Layer\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.depth = d_model // num_heads\n        self.num_heads = num_heads\n\n        self.Wq = Dense(d_model)\n        self.Wk = Dense(d_model)\n        self.Wv = Dense(d_model)\n        self.dense = Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, q, k, v, mask=None):\n        batch_size = tf.shape(q)[0]\n        q = self.split_heads(self.Wq(q), batch_size)\n        k = self.split_heads(self.Wk(k), batch_size)\n        v = self.split_heads(self.Wv(v), batch_size)\n\n        scaled_attention = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n        return self.dense(concat)\n</code></pre></p>"},{"location":"chapter24_transformers/#transformer-encoder-block","title":"Transformer Encoder Block","text":"<pre><code>from tensorflow.keras.layers import LayerNormalization, Dropout, Dense\n\nclass TransformerEncoderBlock(Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = MultiHeadAttention(d_model, num_heads)\n        self.ffn = tf.keras.Sequential([\n            Dense(ff_dim, activation='relu'),\n            Dense(d_model)\n        ])\n        self.layernorm1 = LayerNormalization()\n        self.layernorm2 = LayerNormalization()\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, x, training):\n        attn_output = self.att(x, x, x)\n        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n        ffn_output = self.ffn(out1)\n        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n</code></pre>"},{"location":"chapter24_transformers/#training-transformer-on-text-preview","title":"Training Transformer on Text (Preview)","text":"<p>You can now stack transformer blocks, add embeddings, and train using <code>model.fit()</code> as usual. Later chapters (esp. Part IV and V) show this for:</p> <ul> <li>Sentiment analysis  </li> <li>Text classification  </li> <li>Question answering</li> </ul>"},{"location":"chapter24_transformers/#bert-and-gpt-built-on-transformers","title":"BERT and GPT: Built on Transformers","text":"Model Direction Use Case BERT Bidirectional Classification, QA GPT Left-to-right Generation, Autocomplete <p>You can fine-tune these using <code>transformers</code> from Hugging Face or build them from scratch (see next chapter for applications).</p>"},{"location":"chapter24_transformers/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Understood how attention replaces recurrence  </li> <li>Built a mini transformer encoder using TensorFlow  </li> <li>Explored the components like positional encodings and multi-head attention  </li> <li>Learned how modern models like BERT and GPT are architecturally composed</li> </ul> <p>Transformers are the foundation of state-of-the-art NLP and many vision models. You\u2019ve now walked through the gears that power them.</p>"},{"location":"chapter25_projects/","title":"Chapter 25: NLP Projects \u2014 Spam Detection, Sentiment Analysis, Autocomplete","text":"<p>\u201cLanguage is powerful. Teaching machines to understand it unlocks infinite possibilities.\u201d</p> <p>With your knowledge of tokenization, vectorization, RNNs, and Transformers, you're now ready to build end-to-end NLP applications. In this chapter, we\u2019ll walk through three mini-projects using TensorFlow:</p> <ol> <li>Spam Detection \u2013 Binary classification using classic vectorization</li> <li>Sentiment Analysis \u2013 Text-to-emotion prediction using LSTM</li> <li>Autocomplete \u2013 Next-word prediction using a Transformer</li> </ol> <p>Each project includes:</p> <ul> <li>Data loading  </li> <li>Text preprocessing  </li> <li>Model architecture  </li> <li>Training loop  </li> <li>Evaluation and usage</li> </ul>"},{"location":"chapter25_projects/#1-spam-detection-with-tf-idf","title":"1. Spam Detection with TF-IDF","text":"<p>Dataset: SMS Spam Collection (UCI) <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\", sep='\\t', names=['label', 'message'])\ndf['label'] = df['label'].map({'ham': 0, 'spam': 1})\n</code></pre></p> <p>Preprocessing with TfidfVectorizer <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nX = TfidfVectorizer(max_features=1000).fit_transform(df['message']).toarray()\ny = df['label'].values\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n</code></pre></p> <p>Model <pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(1000,)),\n    Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, validation_split=0.2)\n</code></pre></p>"},{"location":"chapter25_projects/#2-sentiment-analysis-with-lstm","title":"\ud83d\ude0a 2. Sentiment Analysis with LSTM","text":"<p>Dataset: IMDB Reviews <pre><code>(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200)\n</code></pre></p> <p>Model <pre><code>model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=200),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, validation_split=0.2)\n</code></pre></p>"},{"location":"chapter25_projects/#3-autocomplete-with-mini-transformer","title":"3. Autocomplete with Mini Transformer","text":"<p>This is a simplified version, using a small vocabulary.</p> <p>Sample Corpus <pre><code>sentences = [\n    \"hello how are you\",\n    \"hello how is your day\",\n    \"i love natural language processing\",\n    \"tensorflow is powerful\"\n]\n</code></pre></p> <p>Vectorize &amp; Build Transformer</p> <p>Use TextVectorization and custom Transformer layers (as built in Chapter 24). Train it as a language model: for every input sequence, predict the next word.</p>"},{"location":"chapter25_projects/#evaluation-tips","title":"Evaluation Tips","text":"Project Metric Evaluation Example Spam Detection Accuracy, F1 Confusion matrix Sentiment Analysis Accuracy Predict reviews manually Autocomplete Top-k accuracy \u201chello how\u201d \u2192 \u201care\u201d, \u201cis\u201d, \u201cyou\u201d <p>Use TensorBoard to visualize training curves, and checkpoints to save your models.</p>"},{"location":"chapter25_projects/#deployment-ideas","title":"Deployment Ideas","text":"Task Deploy As Spam Detection FastAPI REST API Sentiment Model Hugging Face + Gradio Autocomplete TensorFlow Lite on mobile <p>All three can be integrated into websites, apps, or backend systems for real-time inference.</p>"},{"location":"chapter25_projects/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Built three full NLP projects from start to finish  </li> <li>Used both traditional ML and deep learning  </li> <li>Combined text preprocessing, tokenization, and modeling  </li> <li>Prepared these models for deployment and real-world use</li> </ul> <p>These projects are a stepping stone to larger systems: chatbots, translation, summarization, and beyond.</p>"},{"location":"chapter26_cnn/","title":"Chapter 26: Convolution Layers &amp; CNNs","text":"<p>\u201cIf fully connected layers taught our models to think, convolutional layers taught them to see.\u201d</p>"},{"location":"chapter26_cnn/#introduction-why-convolution","title":"Introduction: Why Convolution?","text":"<p>Imagine trying to recognize a cat in a picture by scanning every pixel individually. That\u2019s what early neural networks did\u2014and they weren\u2019t great at it. But in 1998, LeNet-5 by Yann LeCun changed everything by introducing convolutional layers\u2014special layers that act like visual pattern detectors.</p> <p>Today, Convolutional Neural Networks (CNNs) are the backbone of modern computer vision. From facial recognition to autonomous vehicles, CNNs empower machines to see.</p>"},{"location":"chapter26_cnn/#core-idea-behind-convolution","title":"Core Idea Behind Convolution:","text":"<p>Unlike fully connected layers where each neuron is connected to all inputs, convolution layers use small filters (kernels) to scan over input data. Each filter detects specific features like:</p> <ul> <li>Vertical edges  </li> <li>Horizontal lines  </li> <li>Patterns or textures</li> </ul> <p>This drastically reduces the number of parameters and preserves spatial relationships\u2014crucial for images.</p>"},{"location":"chapter26_cnn/#implementing-convolution-in-tensorflow","title":"Implementing Convolution in TensorFlow","text":"<p>Here's how to use convolutional layers with tf.keras.layers.Conv2D:</p>"},{"location":"chapter26_cnn/#code-example-basic-conv2d-layer","title":"\u2705 Code Example: Basic Conv2D Layer","text":"<p><pre><code>import tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n</code></pre> Explanation</p> <ul> <li><code>Conv2D</code>: 2D convolution layer with 32 filters of size 3x3  </li> <li><code>activation='relu'</code>: adds non-linearity   </li> <li><code>input_shape=(28,28,1)</code>: input is a grayscale 28x28 image  </li> <li><code>MaxPooling2D</code>: downscales the feature map by taking the max value in a 2x2 window  </li> <li><code>Flatten</code> and <code>Dense</code>: used for final classification</li> </ul>"},{"location":"chapter26_cnn/#anatomy-of-a-conv-layer","title":"Anatomy of a Conv Layer","text":"Component Description Filters Small matrices (e.g., 3x3) that slide over the image Stride Steps taken by the filter as it moves Padding Adding borders to maintain output size (e.g., <code>'same'</code>) Activation Usually ReLU to add non-linearity Output Tensor A 3D tensor: (<code>height</code>, <code>width</code>, <code>channels</code>)"},{"location":"chapter26_cnn/#math-behind-convolution","title":"Math Behind Convolution","text":"<p>Given:</p> <ul> <li>Input size: 28x28  </li> <li>Filter size: 3x3  </li> <li>Stride: 1  </li> <li>Padding: 'valid'  </li> <li>Then output size is:</li> </ul> <pre><code>    Output=\u230a (28\u22123+1)/1 \u230b = 26\u00d726\n</code></pre> <p>Each filter generates one feature map. With <code>filters=32</code>, you\u2019ll get 32 feature maps stacked as the output tensor.</p>"},{"location":"chapter26_cnn/#stack-of-convolutions-a-feature-hierarchy","title":"Stack of Convolutions: A Feature Hierarchy","text":"<p>As layers stack up:</p> <ul> <li>First layers detect edges  </li> <li>Middle layers detect patterns (like eyes or wheels)  </li> <li>Later layers detect objects (faces, dogs, cars)</li> </ul>"},{"location":"chapter26_cnn/#try-it-live-google-colab","title":"Try It Live: Google Colab","text":"<p>\u25b6\ufe0f Open in Colab</p>"},{"location":"chapter26_cnn/#summary","title":"Summary","text":"<p>In this chapter, you learned how convolution layers work as the eyes of your model. You saw how:</p> <ul> <li>CNNs drastically reduce parameter count compared to dense networks  </li> <li>Conv2D layers extract features through filters  </li> <li>Feature maps evolve from edges to full object representations</li> </ul>"},{"location":"chapter27_data_augmentation/","title":"Chapter 27: Data Augmentation","text":"<p>\u201cThe best way to generalize is to never see the same image twice.\u201d</p>"},{"location":"chapter27_data_augmentation/#why-augment-data","title":"Why Augment Data?","text":"<p>When training a computer vision model, overfitting is a common enemy\u2014especially when you have limited data. That's where data augmentation steps in. It\u2019s a strategy to artificially expand your dataset by modifying existing images in subtle ways.</p> <p>Think of it like teaching a child to recognize a cat, not just in one pose or lighting, but from every angle, distance, and background.</p>"},{"location":"chapter27_data_augmentation/#types-of-augmentation","title":"Types of Augmentation","text":"<p>Here are common transformations applied in real-time:</p> Augmentation Type Description Flip Horizontal or vertical reflection Rotation Rotating image by a few degrees Zoom Zooming in/out randomly Shift Moving the image along height/width Shear Slanting the image slightly Brightness Randomly adjusting the image lighting Noise Adding small pixel-level disturbances"},{"location":"chapter27_data_augmentation/#tensorflow-implementation-tfkeraspreprocessing-vs-tfimage","title":"TensorFlow Implementation \u2013 <code>tf.keras.preprocessing</code> vs <code>tf.image</code>","text":""},{"location":"chapter27_data_augmentation/#option-1-with-imagedatagenerator-older-api","title":"Option 1: With ImageDataGenerator (older API)","text":"<p><pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n</code></pre> Apply this to your training set: <pre><code>datagen.fit(train_images)\nmodel.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=10)\n</code></pre></p>"},{"location":"chapter27_data_augmentation/#option-2-with-tfkeraslayers-recommended","title":"Option 2: With <code>tf.keras.layers</code> (recommended)","text":"<p><pre><code>from tensorflow.keras import layers, models\n\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    layers.RandomContrast(0.1)\n])\n</code></pre> You can use it like this in your model pipeline: <pre><code>model = tf.keras.Sequential([\n    data_augmentation,\n    layers.Rescaling(1./255),\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    ...\n])\n</code></pre> This is more efficient and GPU-friendly.</p>"},{"location":"chapter27_data_augmentation/#benefits-of-data-augmentation","title":"Benefits of Data Augmentation","text":"<ul> <li>Improves model generalization  </li> <li>Reduces overfitting  </li> <li>Makes training robust to real-world distortions  </li> <li>Requires no extra labeled data</li> </ul>"},{"location":"chapter27_data_augmentation/#best-practices","title":"Best Practices","text":"<ul> <li>Don\u2019t overdo it\u2014too much distortion can confuse the model  </li> <li>Use augmentation only on the training set, never on validation/test  </li> <li>Combine augmentation with regularization (like dropout) for even better results</li> </ul>"},{"location":"chapter27_data_augmentation/#try-it-live-google-colab","title":"Try It Live: Google Colab","text":"<p>\u25b6\ufe0f Colab: Visualize and Apply Data Augmentation (Coming soon to our project repo!)</p>"},{"location":"chapter27_data_augmentation/#summary","title":"Summary","text":"<p>In this chapter, you learned how data augmentation simulates a richer dataset from limited examples. By randomly flipping, rotating, zooming, and more, you can help your CNN see images as if in the wild\u2014not just from a studio.</p>"},{"location":"chapter28_img_class/","title":"Chapter 28: Image Classification","text":"<p>\u201cAt the end of the pipeline, it\u2019s about making a decision\u2014cat or dog, plane or car, tumor or healthy tissue.\u201d</p>"},{"location":"chapter28_img_class/#what-is-image-classification","title":"What Is Image Classification?","text":"<p>Image classification is the task of assigning a label from a predefined set of categories to an input image. Think:</p> <ul> <li>Is this image a cat or a dog?  </li> <li>Does this scan show pneumonia or not?  </li> <li>Which of the 10 digits does this handwritten number represent?</li> </ul> <p>It\u2019s one of the simplest yet most powerful applications of CNNs.</p>"},{"location":"chapter28_img_class/#building-an-end-to-end-classifier-with-tensorflow","title":"Building an End-to-End Classifier with TensorFlow","text":"<p>Let\u2019s classify images from CIFAR-10, a dataset of 60,000 32x32 color images in 10 classes: airplane, car, bird, cat, deer, dog, frog, horse, ship, truck.</p>"},{"location":"chapter28_img_class/#step-1-load-preprocess-the-dataset","title":"Step 1: Load &amp; Preprocess the Dataset","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# Load data\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n# Normalize pixel values to [0, 1]\nx_train, x_test = x_train / 255.0, x_test / 255.0\n</code></pre>"},{"location":"chapter28_img_class/#step-2-define-the-model-architecture","title":"Step 2: Define the Model Architecture","text":"<pre><code>model = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')  # 10 classes\n])\n</code></pre>"},{"location":"chapter28_img_class/#step-3-compile-and-train","title":"Step 3: Compile and Train","text":"<pre><code>model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n</code></pre>"},{"location":"chapter28_img_class/#step-4-evaluate-the-model","title":"Step 4: Evaluate the Model","text":"<pre><code>test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\nprint(f\"Test accuracy: {test_acc:.2f}\")\n</code></pre>"},{"location":"chapter28_img_class/#step-5-make-predictions","title":"Step 5: Make Predictions","text":"<p><pre><code>import numpy as np\n\npredictions = model.predict(x_test)\nprint(\"Predicted label:\", np.argmax(predictions[0]))\n</code></pre> You can match the result to CIFAR-10\u2019s label map: <pre><code>class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n</code></pre></p>"},{"location":"chapter28_img_class/#add-data-augmentation-optional","title":"Add Data Augmentation (Optional)","text":"<p>To improve generalization:</p> <pre><code>data_augmentation = tf.keras.Sequential([\n    layers.RandomFlip('horizontal'),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1)\n])\n\nmodel = models.Sequential([\n    data_augmentation,\n    layers.Rescaling(1./255),\n    ...\n])\n</code></pre>"},{"location":"chapter28_img_class/#metrics-to-monitor","title":"Metrics to Monitor","text":"Metric Why it matters Accuracy Basic measure for classification Confusion Matrix Understand misclassified classes Precision/Recall Especially for imbalanced classes"},{"location":"chapter28_img_class/#try-it-live","title":"Try It Live","text":"<p>\ud83d\udd17 Colab Notebook: CIFAR-10 Classifier (To be linked in the companion repo)</p>"},{"location":"chapter28_img_class/#summary","title":"Summary","text":"<p>In this chapter, you built a complete image classification model using CNNs. You learned to:</p> <ul> <li>Preprocess data and normalize images  </li> <li>Stack convolutional layers effectively  </li> <li>Compile, train, evaluate, and make predictions with TensorFlow </li> </ul>"},{"location":"chapter29_obj_detection/","title":"Chapter 29: Object Detection","text":"<p>\u201cImage classification tells you what\u2014object detection tells you where.\u201d</p>"},{"location":"chapter29_obj_detection/#what-is-object-detection","title":"What Is Object Detection?","text":"<p>Object detection not only classifies what is in an image, but also where it is\u2014by drawing bounding boxes around detected objects.</p> <p>This enables applications like:</p> <ul> <li>Face detection in photos  </li> <li>Pedestrian detection for self-driving cars  </li> <li>Barcode readers in retail  </li> <li>Real-time object tracking in surveillance systems</li> </ul>"},{"location":"chapter29_obj_detection/#classification-vs-detection","title":"Classification vs Detection","text":"Task Input Output Image Classification Image Label (e.g., \"dog\") Object Detection Image Label + Bounding Box <p>Example:</p> <p>Classification: \u201cThere is a cat in the image.\u201d</p> <p>Detection: \u201cThere is a cat at coordinates (x1, y1) to (x2, y2).\u201d</p>"},{"location":"chapter29_obj_detection/#tools-for-object-detection-in-tensorflow","title":"Tools for Object Detection in TensorFlow","text":""},{"location":"chapter29_obj_detection/#option-1-pretrained-models-via-tensorflow-hub","title":"Option 1: Pretrained Models via <code>TensorFlow Hub</code>","text":"<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\n\ndetector = hub.load(\"https://tfhub.dev/google/fasterrcnn/openimages_v4/inception_resnet_v2/1\")\n</code></pre> <p>Input an image:</p> <pre><code>import numpy as np\nfrom PIL import Image\n\ndef load_image(path):\n    img = Image.open(path).resize((512, 512))\n    return np.array(img) / 255.0\n\nimage = load_image('sample.jpg')\nresult = detector(tf.convert_to_tensor([image]), training=False)\n</code></pre>"},{"location":"chapter29_obj_detection/#option-2-tf-object-detection-api-advanced","title":"Option 2: TF Object Detection API (Advanced)","text":"<p>TensorFlow has a dedicated Object Detection API with models like:</p> <ul> <li>SSD (Single Shot MultiBox Detector)  </li> <li>Faster R-CNN  </li> <li>EfficientDet</li> </ul> <p>\u26a0\ufe0f Requires setup with:</p> <ul> <li>Protobuf compilation  </li> <li>COCO-trained checkpoints  </li> <li>Custom label maps</li> </ul>"},{"location":"chapter29_obj_detection/#using-a-pretrained-ssd-model","title":"Using a Pretrained SSD Model","text":"<p>For simpler use cases, try SSD from <code>tf.keras.applications</code> or prepackaged TF Hub models. <pre><code>detector = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n</code></pre></p> <p>Visualize predictions: <pre><code>import matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\n\nimage = tf.image.convert_image_dtype(tf.io.decode_jpeg(tf.io.read_file('cat.jpg')), tf.float32)\nimage_resized = tf.image.resize(image, (320, 320))\n\nresults = detector(tf.expand_dims(image_resized, axis=0))\nprint(results['detection_boxes'])\n</code></pre></p>"},{"location":"chapter29_obj_detection/#visualizing-bounding-boxes","title":"Visualizing Bounding Boxes","text":"<pre><code>import matplotlib.patches as patches\n\ndef draw_boxes(image, boxes):\n    fig, ax = plt.subplots(1)\n    ax.imshow(image)\n    for box in boxes:\n        y1, x1, y2, x2 = box\n        rect = patches.Rectangle((x1*image.shape[1], y1*image.shape[0]), \n                                 (x2 - x1)*image.shape[1], \n                                 (y2 - y1)*image.shape[0], \n                                 linewidth=2, edgecolor='red', facecolor='none')\n        ax.add_patch(rect)\n    plt.show()\n</code></pre>"},{"location":"chapter29_obj_detection/#real-world-applications","title":"Real-World Applications","text":"Industry Application Automotive Pedestrian/vehicle detection Retail Checkout-free shopping Healthcare Tumor boundary detection Manufacturing Faulty component identification"},{"location":"chapter29_obj_detection/#try-it-live","title":"Try It Live","text":"<p>\ud83d\udd17 Colab Notebook: Object Detection with TF Hub (Ready to be plugged into the companion repo)</p>"},{"location":"chapter29_obj_detection/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Learned how object detection adds spatial awareness to classification  </li> <li>Explored TensorFlow Hub and the Object Detection API  </li> <li>Built a pipeline to detect objects and visualize bounding boxes</li> </ul>"},{"location":"chapter2_TF_architecture/","title":"Chapter 2: Architecture of TensorFlow","text":"<p>\u201cSystems that scale don\u2019t happen by accident\u2014they're designed to flow.\u201d</p>"},{"location":"chapter2_TF_architecture/#21-overview-the-core-idea","title":"2.1 Overview: The Core Idea","text":"<p>At its heart, TensorFlow is a computation graph engine. It models operations as nodes, and data (tensors) as edges. This allows it to:  </p> <ul> <li>Represent complex machine learning workflows</li> <li>Optimize execution (via static graphs)</li> <li>Run computations on CPUs, GPUs, TPUs, or even clusters</li> <li>Serialize and deploy models efficiently</li> </ul> <p>It\u2019s not just about training\u2014it\u2019s about building systems that flow across hardware, platforms, and pipelines.</p>"},{"location":"chapter2_TF_architecture/#22-key-components-of-the-tensorflow-architecture","title":"2.2 Key Components of the TensorFlow Architecture","text":"<p>Let\u2019s break down the architecture into digestible layers:</p> <ol> <li> <p>TensorFlow Core (Low-Level APIs)  </p> </li> <li> <p>Handles tensors, operations (ops), device placement, and graphs  </p> </li> <li>Provides full flexibility but requires manual graph building  </li> <li> <p>Ideal for research or custom operation design</p> </li> <li> <p>Eager Execution</p> </li> <li> <p>Default mode in TensorFlow 2.x</p> </li> <li>Immediate evaluation of operations (like Python functions)</li> <li>Easier to debug and experiment with</li> </ol> <p>Note: You can still switch to graph mode using <code>@tf.function</code> for performance.</p> <ol> <li> <p>AutoDiff Engine (tf.GradientTape)  </p> </li> <li> <p>Records operations for automatic differentiation</p> </li> <li> <p>Core for training models via backpropagation</p> </li> <li> <p>tf.data API</p> </li> <li> <p>For input pipelines (batching, shuffling, prefetching)</p> </li> <li>Handles large datasets efficiently</li> <li> <p>Integrates seamlessly with training loops</p> </li> <li> <p>High-Level APIs (tf.keras)</p> </li> <li> <p>Easy model building with Sequential, Model, and layers</p> </li> <li>Abstracts boilerplate training code</li> <li> <p>Integrates with callbacks, metrics, optimizers, etc.</p> </li> <li> <p>Deployment Stack</p> </li> <li> <p>TensorFlow Lite (TFLite): For mobile and embedded</p> </li> <li>TensorFlow.js: For in-browser inference</li> <li>TensorFlow Extended (TFX): For production pipelines</li> <li>TensorFlow Serving: For scalable model APIs</li> </ol>"},{"location":"chapter2_TF_architecture/#23-architecture-diagram-visual-representation","title":"2.3 Architecture Diagram (Visual Representation)","text":"<pre><code>   +----------------------------+\n   |  High-Level APIs (tf.keras)|\n   +----------------------------+\n               |\n   +----------------------------+\n   |      Computation Graph     |\n   |  (tf.function, tf.Tensor)  |\n   +----------------------------+\n               |\n   +----------------------------+\n   |  TensorFlow Core Execution |\n   |     + Device Placement     |\n   |     + AutoDiff Engine      |\n   +----------------------------+\n               |\n   +----------------------------+\n   |  Runtime (CPU, GPU, TPU)   |\n   +----------------------------+\n</code></pre>"},{"location":"chapter2_TF_architecture/#24-why-this-matters","title":"2.4 Why This Matters","text":"<ul> <li>Understanding the architecture unlocks:</li> <li>Better performance (knowing when to use @tf.function)</li> <li>Cleaner code (with tf.data, tf.keras)</li> <li>Easier debugging (stay in eager mode until stable)</li> <li>Smarter deployment (choose TFLite, TF-Serving, or TFJS as needed)</li> </ul> <p>In short: knowing the gears makes you a better TensorFlow mechanic.</p> <p>\u201cBehind every tensor operation is a system designed to scale brains, not just models.\u201d</p>"},{"location":"chapter30_face_mask_dete/","title":"Chapter 30: Face Mask Detection","text":"<p>\u201cWhen vision meets responsibility\u2014models that see and protect.\u201d</p>"},{"location":"chapter30_face_mask_dete/#why-face-mask-detection","title":"\ud83d\ude37 Why Face Mask Detection?","text":"<p>The COVID-19 pandemic highlighted the need for real-time face mask detection in public safety systems. Detecting whether a person is wearing a mask can:</p> <ul> <li>Enhance compliance in public places  </li> <li>Assist automated access control  </li> <li>Provide insights for health monitoring systems</li> </ul> <p>This task is a real-world application of binary object classification layered on top of face detection.</p>"},{"location":"chapter30_face_mask_dete/#problem-breakdown","title":"Problem Breakdown","text":"<p>Detect faces in an image</p> <p>Classify each face:</p> <ul> <li><code>with_mask</code> </li> <li><code>without_mask</code></li> </ul> <p>This is different from simply classifying the whole image\u2014it\u2019s region-specific classification.</p>"},{"location":"chapter30_face_mask_dete/#dataset","title":"Dataset","text":"<p>We\u2019ll use the Face Mask Detection Dataset, which contains:</p> <ul> <li>Labeled images of people with and without masks  </li> <li>Optionally: bounding boxes (if using detection-first approach)</li> </ul> <p>Alternatively, for simplicity, use a folder-structured dataset:</p> <pre><code>dataset/\n  \u251c\u2500\u2500 with_mask/\n  \u2514\u2500\u2500 without_mask/\n</code></pre>"},{"location":"chapter30_face_mask_dete/#implementation-steps","title":"Implementation Steps","text":""},{"location":"chapter30_face_mask_dete/#step-1-load-and-preprocess-images","title":"Step 1: Load and Preprocess Images","text":"<pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\ntrain_gen = datagen.flow_from_directory(\n    'dataset',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nval_gen = datagen.flow_from_directory(\n    'dataset',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n</code></pre>"},{"location":"chapter30_face_mask_dete/#step-2-build-a-cnn-model","title":"Step 2: Build a CNN Model","text":"<pre><code>from tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n    layers.MaxPooling2D(2,2),\n\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D(2,2),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(1, activation='sigmoid')  # Binary classification\n])\n</code></pre>"},{"location":"chapter30_face_mask_dete/#step-3-compile-and-train","title":"Step 3: Compile and Train","text":"<pre><code>model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_gen, validation_data=val_gen, epochs=10)\n</code></pre>"},{"location":"chapter30_face_mask_dete/#step-4-make-predictions","title":"Step 4: Make Predictions","text":"<pre><code>import numpy as np\nfrom tensorflow.keras.preprocessing import image\n\nimg = image.load_img('test.jpg', target_size=(128, 128))\nimg_array = image.img_to_array(img) / 255.0\nimg_array = np.expand_dims(img_array, axis=0)\n\nprediction = model.predict(img_array)[0][0]\nprint(\"With mask\" if prediction &lt; 0.5 else \"Without mask\")\n</code></pre>"},{"location":"chapter30_face_mask_dete/#optional-use-opencv-for-real-time-detection","title":"Optional: Use OpenCV for Real-Time Detection","text":"<p>Integrate with your webcam:</p> <pre><code>import cv2\n\n# Load model and Haar Cascade for face detection\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    _, frame = cap.read()\n    faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n\n    for (x,y,w,h) in faces:\n        face = frame[y:y+h, x:x+w]\n        face_resized = cv2.resize(face, (128,128)) / 255.0\n        prediction = model.predict(np.expand_dims(face_resized, axis=0))[0][0]\n        label = \"With Mask\" if prediction &lt; 0.5 else \"No Mask\"\n        color = (0,255,0) if prediction &lt; 0.5 else (0,0,255)\n        cv2.rectangle(frame, (x,y), (x+w, y+h), color, 2)\n        cv2.putText(frame, label, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n\n    cv2.imshow('Face Mask Detector', frame)\n    if cv2.waitKey(1) == 27: break  # Esc to exit\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"chapter30_face_mask_dete/#deployment-tip","title":"Deployment Tip","text":"<p>Use:</p> <ul> <li>TensorFlow Lite for mobile deployment  </li> <li>FastAPI + OpenCV for web camera APIs  </li> <li>Hugging Face Spaces for demo apps</li> </ul>"},{"location":"chapter30_face_mask_dete/#summary","title":"Summary","text":"<p>In this chapter, you built a binary image classifier to detect face masks using TensorFlow and CNNs. You also explored how to:</p> <ul> <li>Train a model with folder-labeled images  </li> <li>Use real-time webcam feeds for predictions  </li> <li>Visualize and deploy your results</li> </ul>"},{"location":"chapter31_image_segmentation/","title":"Chapter 31: Image Segmentation","text":"<p>\u201cIf object detection draws boxes, image segmentation draws boundaries.\u201d</p>"},{"location":"chapter31_image_segmentation/#what-is-image-segmentation","title":"What Is Image Segmentation?","text":"<p>Image Segmentation is the process of classifying each pixel in an image into a category. Instead of just identifying or locating objects, segmentation understands the exact shape and boundary of each object.</p> <p>There are two main types:</p> Type Description Semantic Segmentation Classifies every pixel (e.g., \"this pixel is part of a cat\") Instance Segmentation Distinguishes between multiple objects of the same class (e.g., \"cat #1\" vs \"cat #2\")"},{"location":"chapter31_image_segmentation/#applications","title":"Applications","text":"<ul> <li> <p>Autonomous Vehicles: Detect roads, pedestrians, signs</p> </li> <li> <p>Medical Imaging: Tumor segmentation in MRIs/CTs</p> </li> <li> <p>Agriculture: Identify plant species in satellite images</p> </li> <li> <p>Robotics: Scene understanding for manipulation</p> </li> </ul>"},{"location":"chapter31_image_segmentation/#dataset-example-oxford-pets","title":"Dataset Example: Oxford Pets","text":"<p>The Oxford-IIIT Pet Dataset includes:</p> <ul> <li>Images of pets (cats/dogs)  </li> <li>Segmentation masks where each pixel is labeled as pet, background, or border</li> </ul> <p>TensorFlow Datasets provides a wrapper: <pre><code>import tensorflow_datasets as tfds\n\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n</code></pre></p>"},{"location":"chapter31_image_segmentation/#building-a-segmentation-model-u-net-style","title":"Building a Segmentation Model (U-Net Style)","text":""},{"location":"chapter31_image_segmentation/#step-1-preprocess-input-labels","title":"Step 1: Preprocess Input &amp; Labels","text":"<pre><code>def normalize(input_image, input_mask):\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n    input_mask = tf.cast(input_mask, tf.uint8)\n    return input_image, input_mask\n</code></pre>"},{"location":"chapter31_image_segmentation/#step-2-u-net-architecture-encoderdecoder","title":"Step 2: U-Net Architecture (Encoder\u2013Decoder)","text":"<pre><code>from tensorflow.keras import layers, models\n\ndef unet_model(output_channels):\n    inputs = layers.Input(shape=[128, 128, 3])\n\n    # Encoder\n    down1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    down1 = layers.MaxPooling2D()(down1)\n\n    down2 = layers.Conv2D(128, 3, activation='relu', padding='same')(down1)\n    down2 = layers.MaxPooling2D()(down2)\n\n    # Bottleneck\n    bottleneck = layers.Conv2D(256, 3, activation='relu', padding='same')(down2)\n\n    # Decoder\n    up1 = layers.UpSampling2D()(bottleneck)\n    up1 = layers.Concatenate()([up1, down2])\n    up1 = layers.Conv2D(128, 3, activation='relu', padding='same')(up1)\n\n    up2 = layers.UpSampling2D()(up1)\n    up2 = layers.Concatenate()([up2, down1])\n    up2 = layers.Conv2D(64, 3, activation='relu', padding='same')(up2)\n\n    outputs = layers.Conv2D(output_channels, 1, activation='softmax')(up2)\n\n    return models.Model(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"chapter31_image_segmentation/#step-3-train-the-model","title":"Step 3: Train the Model","text":"<pre><code>model = unet_model(output_channels=3)  # background, object, border\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(train_batches, epochs=20, validation_data=val_batches)\n</code></pre>"},{"location":"chapter31_image_segmentation/#visualize-the-segmentation-output","title":"Visualize the Segmentation Output","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef display(display_list):\n    plt.figure(figsize=(15, 5))\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\n</code></pre>"},{"location":"chapter31_image_segmentation/#evaluation-tips","title":"Evaluation Tips","text":"<ul> <li>Use IoU (Intersection over Union) for pixel-level accuracy  </li> <li>Use Dice coefficient for imbalanced segmentation tasks  </li> <li>Visually inspect masks to verify semantic alignment</li> </ul>"},{"location":"chapter31_image_segmentation/#try-it-live","title":"Try It Live","text":"<p>\ud83d\udd17 Colab: U-Net Semantic Segmentation (Coming soon to the repo: semantic segmentation playground with custom uploads!)</p>"},{"location":"chapter31_image_segmentation/#summary","title":"Summary","text":"<p>In this chapter, you:</p> <ul> <li>Understood the difference between classification, detection, and segmentation  </li> <li>Built a U-Net-based semantic segmentation model in TensorFlow  </li> <li>Visualized how the model learns to identify object boundaries, pixel by pixel</li> </ul>"},{"location":"chapter32_GANs/","title":"Chapter 32: GANs for Image Generation","text":"<p>\u201cTwo networks, locked in a dance. One imagines, the other critiques. Together, they create.\u201d</p>"},{"location":"chapter32_GANs/#what-are-gans","title":"What Are GANs?","text":"<p>Generative Adversarial Networks (GANs) are a class of deep learning models invented by Ian Goodfellow in 2014. Their purpose: generate new, realistic data from noise.</p> <p>A GAN consists of two neural networks:</p> <ul> <li>Generator (G): Learns to create realistic images  </li> <li>Discriminator (D): Learns to tell real from fake images</li> </ul> <p>These two models are trained in a zero-sum game:</p> <ul> <li>The Generator tries to fool the Discriminator  </li> <li>The Discriminator tries to catch the Generator\u2019s fakes</li> </ul> <p>Over time, both improve until the generated images are indistinguishable from real ones.</p>"},{"location":"chapter32_GANs/#gan-architecture-overview","title":"GAN Architecture Overview","text":"<pre><code>Noise (z) \u2192 Generator \u2192 Fake Image\n                     \u2193\n        Discriminator \u2190 Real Image\n</code></pre> Component Purpose Generator Converts random noise into images Discriminator Judges real vs fake images Training Minimax game until equilibrium"},{"location":"chapter32_GANs/#dataset-mnist-for-simplicity","title":"Dataset: MNIST for Simplicity","text":"<p>We\u2019ll use the MNIST dataset of handwritten digits for training a simple GAN.</p> <pre><code>(x_train, _), _ = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(-1, 28, 28, 1).astype('float32')\nx_train = (x_train - 127.5) / 127.5  # Normalize to [-1, 1]\n</code></pre>"},{"location":"chapter32_GANs/#build-the-generator","title":"Build the Generator","text":"<pre><code>from tensorflow.keras import layers\n\ndef build_generator():\n    model = tf.keras.Sequential([\n        layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        layers.Reshape((7, 7, 256)),\n        layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh')\n    ])\n    return model\n</code></pre>"},{"location":"chapter32_GANs/#build-the-discriminator","title":"Build the Discriminator","text":"<pre><code>def build_discriminator():\n    model = tf.keras.Sequential([\n        layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n\n        layers.Conv2D(128, (5,5), strides=(2,2), padding='same'),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n\n        layers.Flatten(),\n        layers.Dense(1)\n    ])\n    return model\n</code></pre>"},{"location":"chapter32_GANs/#training-the-gan","title":"Training the GAN","text":"<pre><code>cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# Loss for discriminator\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    return real_loss + fake_loss\n\n# Loss for generator\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n</code></pre>"},{"location":"chapter32_GANs/#optimizers","title":"Optimizers","text":"<pre><code>generator = build_generator()\ndiscriminator = build_discriminator()\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n</code></pre>"},{"location":"chapter32_GANs/#training-loop","title":"Training Loop","text":"<pre><code>import time\n\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, 100])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated = generator(noise, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_gen, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_disc, discriminator.trainable_variables))\n</code></pre>"},{"location":"chapter32_GANs/#visualizing-generated-images","title":"Visualizing Generated Images","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef generate_and_plot_images(model, epoch, test_input):\n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(4,4))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n\n    plt.show()\n</code></pre>"},{"location":"chapter32_GANs/#try-it-live","title":"Try It Live","text":"<p>\ud83d\udd17 Colab: MNIST GAN from Scratch (Will be added soon, to my companion repo for users to play with and train their own generators!)</p>"},{"location":"chapter32_GANs/#summary","title":"Summary","text":"<p>In this chapter, you learned how GANs:</p> <ul> <li>Work through adversarial training of Generator and Discriminator  </li> <li>Can generate realistic images from noise  </li> <li>Are implemented step-by-step using TensorFlow</li> </ul> <p>GANs are a bridge between art and intelligence\u2014used in face generation, style transfer, image-to-image translation, super-resolution, and more.</p>"},{"location":"chapter33_time_series/","title":"Chapter 33: Time Series Forecasting","text":"<p>\u201cThe best way to predict the future is to model the past \u2014 and time series helps us do just that.\u201d</p>"},{"location":"chapter33_time_series/#introduction-when-time-becomes-a-feature","title":"\ud83e\udded Introduction: When Time Becomes a Feature","text":"<p>Time series data is everywhere \u2014 stock prices, weather data, electricity usage, web traffic, heart rate signals, you name it. What makes time series special is that order matters. Unlike other datasets, a time series isn\u2019t just a collection of independent observations \u2014 it\u2019s a sequence, where each point is dependent on the ones before it.</p> <p>This chapter introduces how to handle time series forecasting using TensorFlow. We\u2019ll move from simple concepts like trend and seasonality, all the way to training neural networks (including LSTM and CNN variants) to make future predictions.</p>"},{"location":"chapter33_time_series/#key-concepts-in-time-series","title":"Key Concepts in Time Series","text":"<p>Before we jump into TensorFlow code, let\u2019s quickly review the building blocks of time series:</p> <ul> <li>Trend: Long-term increase or decrease in the data  </li> <li>Seasonality: Repeating patterns over a fixed period (e.g., daily, monthly)  </li> <li>Noise: Randomness that we want to filter out  </li> <li>Lag: How past values affect future values (e.g., y(t) = y(t-1) + \u03b5)</li> </ul>"},{"location":"chapter33_time_series/#data-preparation","title":"Data Preparation","text":"<p>Here\u2019s how to prepare a time series dataset for training:</p>"},{"location":"chapter33_time_series/#step-1-load-the-data","title":"Step 1: Load the Data","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv')\ndf.columns = ['Month', 'Passengers']\ndf['Month'] = pd.to_datetime(df['Month'])\ndf.set_index('Month', inplace=True)\n\ndf.plot()\nplt.title(\"Monthly Airline Passengers\")\nplt.show()\n</code></pre>"},{"location":"chapter33_time_series/#step-2-normalize-window-the-data","title":"Step 2: Normalize &amp; Window the Data","text":"<p>TensorFlow expects a supervised format \u2014 so we convert time series into windows of inputs and labels. <pre><code>import numpy as np\n\ndef create_dataset(series, window_size):\n    X, y = [], []\n    for i in range(len(series) - window_size):\n        X.append(series[i:i+window_size])\n        y.append(series[i+window_size])\n    return np.array(X), np.array(y)\n\nseries = df['Passengers'].values.astype(np.float32)\nseries = (series - series.mean()) / series.std()  # normalize\nwindow_size = 12\n\nX, y = create_dataset(series, window_size)\nX = X[..., np.newaxis]  # [batch, time, features]\n</code></pre></p>"},{"location":"chapter33_time_series/#model-1-dense-neural-network","title":"Model 1: Dense Neural Network","text":"<p>Let\u2019s start with a simple fully-connected network.</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = tf.keras.Sequential([\n    layers.Input(shape=(window_size, 1)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, y, epochs=100, verbose=1)\n</code></pre>"},{"location":"chapter33_time_series/#model-2-lstm-for-time-awareness","title":"Model 2: LSTM for Time Awareness","text":"<p>Recurrent models like LSTM are specifically designed for sequences. <pre><code>model = tf.keras.Sequential([\n    layers.Input(shape=(window_size, 1)),\n    layers.LSTM(64),\n    layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X, y, epochs=100)\n</code></pre></p>"},{"location":"chapter33_time_series/#visualizing-the-prediction","title":"\ud83d\udcc8 Visualizing the Prediction","text":"<p>After training, you can compare predictions to the actual values. <pre><code>pred = model.predict(X)\n\nplt.plot(series[window_size:], label='Actual')\nplt.plot(pred.squeeze(), label='Predicted')\nplt.legend()\nplt.title(\"Forecast vs Actual\")\nplt.show()\n</code></pre></p>"},{"location":"chapter33_time_series/#alternative-models-youll-explore-later","title":"Alternative Models You\u2019ll Explore Later","text":"<ul> <li>1D CNNs: For fast inference on long sequences  </li> <li>Transformer Time Series Models: For long-term memory  </li> <li>Autoregressive RNNs: Predict step-by-step into the future  </li> <li>Hybrid Models: Combine statistical models (ARIMA) with DL</li> </ul>"},{"location":"chapter33_time_series/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>How time series forecasting differs from other prediction tasks  </li> <li>How to preprocess and window time series data  </li> <li>How to train Dense and LSTM models using TensorFlow  </li> <li>How to evaluate and visualize your predictions</li> </ul> <p>Time series forecasting is one of the most powerful ML tools for decision-making in business, finance, and operations. As we build on this, you\u2019ll be ready to tackle real datasets like Bitcoin prices, sales prediction, or even traffic flow \u2014 with TensorFlow as your lens into the future.</p>"},{"location":"chapter34_recom_systems/","title":"Chapter 34: Recommender Systems","text":"<p>\u201cThe best suggestions are those you didn\u2019t even know you needed \u2014 until the machine learned them.\u201d</p>"},{"location":"chapter34_recom_systems/#introduction-behind-every-you-may-also-like","title":"\ud83c\udfac Introduction: Behind Every \u201cYou May Also Like\u2026\u201d","text":"<p>From Netflix suggesting your next binge to Amazon nudging your next impulse buy, recommender systems are the engine behind personalized digital experiences. They model your behavior, compare it with others, and predict what you\u2019ll likely enjoy next.</p> <p>In this chapter, we\u2019ll explore how to build recommender systems in TensorFlow \u2014 starting from the simplest collaborative filtering techniques to deep learning-powered ranking models using the tf.recommendation ecosystem and TensorFlow Recommenders (TFRS).</p>"},{"location":"chapter34_recom_systems/#types-of-recommender-systems","title":"Types of Recommender Systems","text":"<p>There are three core approaches:</p> <ul> <li> <p>Content-Based Filtering Recommends items similar to ones the user liked (based on item features)</p> </li> <li> <p>Collaborative Filtering Recommends based on user-item interactions (e.g., matrix factorization)</p> </li> <li> <p>Hybrid Systems Combine both worlds \u2014 Netflix is a classic example</p> </li> </ul>"},{"location":"chapter34_recom_systems/#dataset-movielens-mini","title":"Dataset: MovieLens Mini","text":"<p>We'll use the MovieLens 100k dataset \u2014 a standard for testing recommender systems. <pre><code>!pip install -q tensorflow-recommenders\n!pip install -q tensorflow-datasets\n\nimport tensorflow_datasets as tfds\nimport tensorflow_recommenders as tfrs\nimport tensorflow as tf\n\nratings = tfds.load('movielens/100k-ratings', split='train')\nmovies = tfds.load('movielens/100k-movies', split='train')\n</code></pre></p>"},{"location":"chapter34_recom_systems/#preprocessing-the-data","title":"Preprocessing the Data","text":"<pre><code>ratings = ratings.map(lambda x: {\n    \"movie_title\": x[\"movie_title\"],\n    \"user_id\": x[\"user_id\"]\n})\n\nmovie_titles = movies.map(lambda x: x[\"movie_title\"])\n\n# Vocabulary for embedding\nuser_ids = ratings.map(lambda x: x[\"user_id\"])\nmovie_titles = ratings.map(lambda x: x[\"movie_title\"])\n\nunique_user_ids = np.unique(list(user_ids.as_numpy_iterator()))\nunique_movie_titles = np.unique(list(movie_titles.as_numpy_iterator()))\n</code></pre>"},{"location":"chapter34_recom_systems/#building-the-model","title":"Building the Model","text":""},{"location":"chapter34_recom_systems/#1-user-movie-embedding-models","title":"1. User &amp; Movie Embedding Models","text":"<pre><code>embedding_dim = 32\n\nuser_model = tf.keras.Sequential([\n    tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n    tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dim)\n])\n\nmovie_model = tf.keras.Sequential([\n    tf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\n    tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dim)\n])\n</code></pre>"},{"location":"chapter34_recom_systems/#2-metric-learning-model-with-tfrs","title":"2. Metric Learning Model with TFRS","text":"<pre><code>class MovieModel(tfrs.Model):\n\n    def __init__(self, user_model, movie_model):\n        super().__init__()\n        self.user_model = user_model\n        self.movie_model = movie_model\n        self.task = tfrs.tasks.Retrieval(\n            metrics=tfrs.metrics.FactorizedTopK(\n                candidates=movie_titles.batch(128).map(movie_model)\n            )\n        )\n\n    def compute_loss(self, features, training=False):\n        user_embeddings = self.user_model(features[\"user_id\"])\n        movie_embeddings = self.movie_model(features[\"movie_title\"])\n        return self.task(user_embeddings, movie_embeddings)\n</code></pre>"},{"location":"chapter34_recom_systems/#training","title":"Training","text":"<pre><code>model = MovieModel(user_model, movie_model)\nmodel.compile()\n\ncached_ratings = ratings.shuffle(100_000).batch(8192).cache()\n\nmodel.fit(cached_ratings, epochs=3)\n</code></pre>"},{"location":"chapter34_recom_systems/#making-recommendations","title":"Making Recommendations","text":"<pre><code>index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\nindex.index_from_dataset(\n    movie_titles.batch(100).map(lambda title: (title, model.movie_model(title)))\n)\n\n# Predict for a user\nuser_id = \"42\"\nscores, titles = index(tf.constant([user_id]))\nprint(f\"Recommendations for user {user_id}: {titles[0, :3].numpy()}\")\n</code></pre>"},{"location":"chapter34_recom_systems/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>The difference between collaborative, content-based, and hybrid recommendation systems  </li> <li>How to use the TensorFlow Recommenders (TFRS) library  </li> <li>How to train an end-to-end retrieval model using user and item embeddings  </li> <li>How to generate personalized recommendations</li> </ul> <p>Recommender systems are deeply embedded in modern software ecosystems \u2014 from e-commerce to social media. By learning how to build one, you gain the ability to create truly personalized user experiences.</p>"},{"location":"chapter35_TF_Lite/","title":"Chapter 35: TensorFlow Lite &amp; Mobile Deployment","text":"<p>\u201cA model isn\u2019t truly intelligent until it fits in your pocket.\u201d</p>"},{"location":"chapter35_TF_Lite/#introduction-why-deploy-on-mobile","title":"\ud83d\udcf1 Introduction: Why Deploy on Mobile?","text":"<p>TensorFlow Lite (TFLite) is TensorFlow\u2019s lightweight version tailored for mobile and edge devices like Android, iOS, Raspberry Pi, and microcontrollers. With the rise of real-time AI on phones (face unlock, voice assistants, image classifiers), deploying efficient models to run locally is now a core part of ML engineering.</p> <p>In this chapter, you\u2019ll learn how to:</p> <ul> <li>Convert TensorFlow models to <code>.tflite</code> </li> <li>Optimize for performance with quantization  </li> <li>Deploy and test on Android/iOS  </li> <li>Run inference in Python or embedded systems</li> </ul>"},{"location":"chapter35_TF_Lite/#step-1-convert-a-tensorflow-model-to-tflite","title":"Step 1: Convert a TensorFlow Model to TFLite","text":"<p>We\u2019ll begin with a simple image classifier (e.g., trained on MNIST or MobileNet). <pre><code>import tensorflow as tf\n\n# Load or train a model (example: MobileNetV2 pretrained)\nmodel = tf.keras.applications.MobileNetV2(weights=\"imagenet\", input_shape=(224, 224, 3))\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model\nwith open(\"mobilenetv2.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n</code></pre></p>"},{"location":"chapter35_TF_Lite/#step-2-optimize-the-model-with-quantization","title":"Step 2: Optimize the Model with Quantization","text":"<p>Reducing model size and improving speed, especially for devices with limited resources. <pre><code># Dynamic Range Quantization (fastest + simplest)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_model = converter.convert()\n\nwith open(\"mobilenetv2_quant.tflite\", \"wb\") as f:\n    f.write(quantized_model)\n</code></pre> Other options:</p> <ul> <li> <p>Post-training integer quantization</p> </li> <li> <p>Full integer quantization (with representative dataset)</p> </li> <li> <p>Float16 quantization for GPUs</p> </li> </ul>"},{"location":"chapter35_TF_Lite/#step-3-run-inference-with-tflite-interpreter-python","title":"Step 3: Run Inference with TFLite Interpreter (Python)","text":"<pre><code>import numpy as np\nfrom PIL import Image\n\n# Load TFLite model\ninterpreter = tf.lite.Interpreter(model_path=\"mobilenetv2_quant.tflite\")\ninterpreter.allocate_tensors()\n\n# Get input/output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Prepare image input\nimg = Image.open(\"cat.jpg\").resize((224, 224))\ninput_data = np.expand_dims(np.array(img, dtype=np.float32) / 255.0, axis=0)\n\n# Run inference\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\noutput_data = interpreter.get_tensor(output_details[0]['index'])\n\nprint(\"Prediction:\", np.argmax(output_data))\n</code></pre>"},{"location":"chapter35_TF_Lite/#step-4-deploy-to-androidios","title":"\ud83d\udcf2 Step 4: Deploy to Android/iOS","text":"<p>You can now take your <code>.tflite</code> model and integrate it into:</p> <ul> <li> <p>Android Studio using <code>TensorFlow Lite Task Library</code> or <code>Interpreter API</code></p> </li> <li> <p>iOS apps using Swift or Objective-C</p> </li> <li> <p>Flutter apps using the <code>tflite_flutter</code> plugin</p> </li> </ul> <p>\ud83d\udca1 Bonus: Use ML Model Binding in Android Studio for no-code integration of <code>.tflite</code> models with image or text inputs!</p>"},{"location":"chapter35_TF_Lite/#other-deployment-targets","title":"Other Deployment Targets","text":"<p>TensorFlow Lite isn\u2019t just for phones:</p> Platform Use Case Raspberry Pi Smart cameras, IoT vision Coral Edge TPU Ultra-fast inference with acceleration Microcontrollers TinyML with TensorFlow Lite for Microcontrollers"},{"location":"chapter35_TF_Lite/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li> <p>How to convert Keras models into efficient .tflite files</p> </li> <li> <p>How to optimize models with quantization</p> </li> <li> <p>How to run inference in Python, Android, or iOS</p> </li> <li> <p>That TFLite opens the door to AI at the edge \u2014 fast, offline, and private</p> </li> </ul> <p>TensorFlow Lite brings ML to environments where connectivity and power are limited, but the need for real-time intelligence is high \u2014 from smartwatches and cameras to drones and industrial machines.</p>"},{"location":"chapter36_TFX/","title":"Chapter 36: TensorFlow Extended (TFX) for Production Pipelines","text":"<p>\u201cModels aren\u2019t magic \u2014 they\u2019re the output of a pipeline that\u2019s reproducible, testable, and scalable.\u201d</p>"},{"location":"chapter36_TFX/#introduction-from-experiment-to-deployment","title":"Introduction: From Experiment to Deployment","text":"<p>Training a model in a notebook is one thing \u2014 deploying it at scale in a robust production environment is a whole different story. That\u2019s where TFX (TensorFlow Extended) comes in.</p> <p>TFX is an end-to-end ML platform for production-grade ML pipelines built around TensorFlow. It handles:</p> <ul> <li> <p>Data ingestion</p> </li> <li> <p>Data validation</p> </li> <li> <p>Feature engineering</p> </li> <li> <p>Model training</p> </li> <li> <p>odel validation</p> </li> <li> <p>Deployment</p> </li> </ul> <p>All with pipeline reproducibility, CI/CD, and version control baked in.</p>"},{"location":"chapter36_TFX/#key-components-of-a-tfx-pipeline","title":"Key Components of a TFX Pipeline","text":"Component Purpose ExampleGen Ingest and split raw data (CSV, TFRecord, etc.) StatisticsGen Computes statistics over data SchemaGen Infers data schema from stats ExampleValidator Detects anomalies and missing values Transform Performs feature engineering Trainer Trains a model using TensorFlow Evaluator Measures model quality (blessing or rejection) Pusher Pushes the model to serving environment"},{"location":"chapter36_TFX/#example-building-a-tfx-pipeline-basic","title":"Example: Building a TFX Pipeline (Basic)","text":"<pre><code>pip install -q tfx\n</code></pre>"},{"location":"chapter36_TFX/#step-1-directory-setup","title":"Step 1: Directory Setup","text":"<pre><code>import os\n\nPIPELINE_NAME = \"sentiment_pipeline\"\nPIPELINE_ROOT = os.path.join(\"pipelines\", PIPELINE_NAME)\nMETADATA_PATH = os.path.join(\"metadata\", PIPELINE_NAME, \"metadata.db\")\n</code></pre>"},{"location":"chapter36_TFX/#define-pipeline-components","title":"Define Pipeline Components","text":"<p><pre><code>from tfx.components import CsvExampleGen\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\ncontext = InteractiveContext()\n\nexample_gen = CsvExampleGen(input_base=\"data/\")\ncontext.run(example_gen)\n</code></pre> You can visualize the generated schema, examples, and anomalies with TensorBoard or Jupyter.</p>"},{"location":"chapter36_TFX/#step-3-feature-engineering-and-model-training","title":"Step 3: Feature Engineering and Model Training","text":"<pre><code>from tfx.components import Transform, Trainer\nfrom tfx.proto import trainer_pb2\n\ntrainer = Trainer(\n    module_file='model.py',  # Your model logic here\n    examples=example_gen.outputs['examples'],\n    train_args=trainer_pb2.TrainArgs(num_steps=1000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=500)\n)\ncontext.run(trainer)\n</code></pre>"},{"location":"chapter36_TFX/#step-4-evaluating-and-pushing","title":"Step 4: Evaluating and Pushing","text":"<pre><code>from tfx.components import Evaluator, Pusher\n\nevaluator = Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model']\n)\n\npusher = Pusher(\n    model=trainer.outputs['model'],\n    push_destination=tfx.proto.pusher_pb2.PushDestination(\n        filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n            base_directory=\"serving_model/\"\n        )\n    )\n)\ncontext.run(pusher)\n</code></pre>"},{"location":"chapter36_TFX/#optional-orchestration-tools","title":"Optional: Orchestration Tools","text":"<p>TFX integrates with orchestrators like:</p> <ul> <li> <p>Apache Airflow</p> </li> <li> <p>Kubeflow Pipelines</p> </li> <li> <p>Vertex AI Pipelines (GCP)</p> </li> <li> <p>Dagster (community)</p> </li> </ul> <p>This lets you schedule, version, and monitor your pipelines in production environments.</p>"},{"location":"chapter36_TFX/#bonus-features","title":"Bonus Features","text":"<p>Model Blessing: Only deploy models that pass thresholds</p> <p>CI/CD for ML: Automate training, evaluation, and deployment</p> <p>ML Metadata Tracking: Reproducibility and lineage</p> <p>TensorBoard Integration: For monitoring and debugging</p>"},{"location":"chapter36_TFX/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li> <p>What TensorFlow Extended (TFX) is and why it matters in production</p> </li> <li> <p>How to build a simple TFX pipeline: ingest \u2192 validate \u2192 train \u2192 deploy</p> </li> <li> <p>How to scale with orchestration and CI/CD tools</p> </li> <li> <p>How TFX promotes reliability, observability, and reproducibility in ML workflows</p> </li> </ul> <p>TFX is your bridge between research and reality. It ensures your models are not only accurate, but also trusted, trackable, and repeatable in the messy world of production systems.</p>"},{"location":"chapter37_TF_HuggingFace/","title":"Chapter 37: Integrating TensorFlow with Hugging Face","text":"<p>\u201cWhen TensorFlow meets Hugging Face, models gain both muscle and memory.\u201d</p>"},{"location":"chapter37_TF_HuggingFace/#introduction-the-best-of-both-worlds","title":"Introduction: The Best of Both Worlds","text":"<ul> <li> <p>TensorFlow is an industrial-grade engine. Hugging Face is a model zoo with superpowers. Combine the two and you get:</p> </li> <li> <p>Access to thousands of pretrained models (BERT, GPT-2, ViT, T5, etc.)</p> </li> <li> <p>Seamless tokenization, loading, and fine-tuning</p> </li> <li> <p>Exportable <code>.h5</code>, <code>.pb</code>, or <code>.tflite</code> models for real-world deployment</p> </li> </ul> <p>In this chapter, we\u2019ll walk through how to use Hugging Face Transformers with TensorFlow, including:</p> <ul> <li> <p>Loading pretrained models</p> </li> <li> <p>Tokenizing data</p> </li> <li> <p>Fine-tuning for NLP tasks</p> </li> <li> <p>Saving &amp; exporting for inference</p> </li> </ul>"},{"location":"chapter37_TF_HuggingFace/#step-1-install-the-libraries","title":"Step 1: Install the Libraries","text":"<pre><code>pip install -q transformers datasets\n</code></pre>"},{"location":"chapter37_TF_HuggingFace/#step-2-load-a-pretrained-bert-model","title":"Step 2: Load a Pretrained BERT Model","text":"<p>Let\u2019s classify sentiment using <code>bert-base-uncased</code>.</p> <pre><code>from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\nimport tensorflow as tf\n\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n</code></pre>"},{"location":"chapter37_TF_HuggingFace/#step-3-prepare-data-eg-imdb-sentiment","title":"Step 3: Prepare Data (e.g., IMDb Sentiment)","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"imdb\")\ntrain_texts = dataset[\"train\"][\"text\"][:3000]\ntrain_labels = dataset[\"train\"][\"label\"][:3000]\n\n# Tokenize\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='tf')\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n)).batch(16)\n</code></pre>"},{"location":"chapter37_TF_HuggingFace/#step-4-compile-and-fine-tune","title":"Step 4: Compile and Fine-Tune","text":"<pre><code>optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\nmodel.fit(train_dataset, epochs=2)\n</code></pre>"},{"location":"chapter37_TF_HuggingFace/#step-5-save-and-export","title":"Step 5: Save and Export","text":"<p>Save the model (TensorFlow format) <pre><code>model.save_pretrained(\"bert-finetuned-imdb\")\ntokenizer.save_pretrained(\"bert-finetuned-imdb\")\n</code></pre></p> <p>You can now:</p> <ul> <li> <p>Reload it via <code>TFAutoModelForSequenceClassification.from_pretrained()</code></p> </li> <li> <p>Convert to <code>.tflite</code> using <code>TFLiteConverter</code></p> </li> <li> <p>Deploy on Hugging Face Spaces or use <code>transformers.pipelines</code> for inference</p> </li> </ul>"},{"location":"chapter37_TF_HuggingFace/#bonus-quick-prediction-with-pipeline","title":"Bonus: Quick Prediction with <code>pipeline()</code>","text":"<pre><code>from transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", model=\"bert-finetuned-imdb\", tokenizer=\"bert-finetuned-imdb\")\nprint(classifier(\"I loved this movie! So well-acted and emotional.\"))\n</code></pre>"},{"location":"chapter37_TF_HuggingFace/#summary","title":"Summary","text":"<p>In this chapter, you learned:</p> <ul> <li> <p>How to load and fine-tune Hugging Face models using TensorFlow</p> </li> <li> <p>How to tokenize text and structure datasets for training</p> </li> <li> <p>How to export models and deploy them in real-world apps</p> </li> </ul> <p>TensorFlow\u2019s maturity in deployment combined with Hugging Face\u2019s innovation in pretrained models gives you the ultimate toolkit \u2014 powerful models with plug-and-play simplicity.</p> <p>Whether you\u2019re building sentiment analyzers, question-answering bots, or vision transformers, this integration helps you stand on the shoulders of AI giants.</p>"},{"location":"chapter3_tensorflow_vs_keras/","title":"Chapter 3: TensorFlow vs Keras","text":"<p>\u201cAbstraction isn\u2019t just convenience\u2014it\u2019s control disguised as simplicity.\u201d</p>"},{"location":"chapter3_tensorflow_vs_keras/#31-the-relationship-is-keras-part-of-tensorflow","title":"3.1 The Relationship: Is Keras Part of TensorFlow?","text":"<p>Short answer: Yes. But not always. Keras started as an independent deep learning API in 2015, aiming to make neural networks user-friendly. Originally, it supported multiple backends like TensorFlow, Theano, and CNTK.</p> <p>But in 2017, Google integrated Keras as TensorFlow\u2019s official high-level API, now accessed as: <pre><code>import tensorflow as tf\nfrom tensorflow import keras\n</code></pre> You\u2019ll often see it written as tf.keras\u2014that\u2019s the TensorFlow-native version. And it\u2019s not just a wrapper anymore\u2014it\u2019s part of TensorFlow\u2019s core ecosystem.</p>"},{"location":"chapter3_tensorflow_vs_keras/#32-key-differences-and-integration-points","title":"3.2 Key Differences and Integration Points","text":"<p>Let\u2019s compare them side by side:</p> Feature tf.keras Standalone Keras (pip install keras) Backend Engine TensorFlow only Can run on TensorFlow, Theano, etc. Performance Optimizations Supports @tf.function, XLA, mixed-precision Limited (no deep TF graph integration) Distributed Training Integrated with tf.distribute.Strategy Manual or not available Mobile Deployment TFLite-compatible Not directly Versioning Follows TensorFlow versioning Versioned independently Recommended? \u2705 Yes, always use tf.keras today \u274c Deprecated for most TF users"},{"location":"chapter3_tensorflow_vs_keras/#33-why-tensorflow-adopted-keras","title":"3.3 Why TensorFlow Adopted Keras","text":"<p>Because building models directly with low-level TensorFlow APIs was... painful. <pre><code># Old-school TF 1.x style (yikes)\nW = tf.Variable(tf.random.normal((784, 64)))\nb = tf.Variable(tf.zeros(64))\ny = tf.nn.relu(tf.matmul(x, W) + b)\n</code></pre></p> <p>Now with <code>tf.keras</code>: <pre><code>from tensorflow.keras import layers\n\nmodel = tf.keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(784,))\n])\n</code></pre> Cleaner. Safer. Reusable. And it connects directly to:  </p> <ul> <li>Loss functions (tf.keras.losses)  </li> <li>Optimizers (tf.keras.optimizers)  </li> <li>Metrics (tf.keras.metrics)  </li> <li>Model training (model.fit, model.evaluate, etc.)</li> </ul>"},{"location":"chapter3_tensorflow_vs_keras/#34-the-layer-cake-of-tensorflow","title":"3.4 The Layer Cake of TensorFlow","text":"<pre><code>     [ tf.keras (user API) ]  \n                \u2193  \n  [ Computation Graph (tf.function) ]  \n                \u2193  \n    [ Core TensorFlow Engine ]  \n                \u2193  \n  [ Hardware Execution (CPU/GPU/TPU) ]  \n</code></pre> <p>The beauty of <code>tf.keras</code> is this: you stay at the top layer, and TensorFlow handles the heavy machinery underneath.</p> <p>You can still drop down to lower levels when needed\u2014but <code>tf.keras</code> is designed for 90% of use cases.</p>"},{"location":"chapter3_tensorflow_vs_keras/#35-when-to-use-tfkeras-vs-raw-tensorflow","title":"3.5 When to Use tf.keras vs Raw TensorFlow","text":"Situation Use tf.keras? Use raw tf? Building standard neural nets \u2705 Yes \u274c Too verbose Custom ops / experimental graph behavior \u274c Maybe \u2705 Yes Writing production models \u2705 Yes \u2705 Sometimes Debugging gradients manually \u2705 + GradientTape \u2705 <p>Use <code>tf.keras</code> unless you're experimenting with internals or need full control.</p> <p>\u201cKeras makes TensorFlow human\u2014so you can focus on ideas, not boilerplate.\u201d</p>"},{"location":"chapter4_installation_and_setup/","title":"Chapter 4: Installation &amp; Setup","text":"<p>\u201cA neural net\u2019s journey begins with a single tensor.\u201d</p>"},{"location":"chapter4_installation_and_setup/#41-preparing-your-workspace","title":"4.1 Preparing Your Workspace","text":"<p>Let\u2019s keep things clean and self-contained. You\u2019ll be using a virtual environment inside your TensorFlow folder for local experimentation.</p> <p>\u2705 Step-by-step:</p> <p>step 1. Navigate to your project folder <pre><code>    cd C:\\Users\\Clay\\Desktop\\Tutorials\\TensorFlow\n</code></pre> step 2. Create a virtual environment <pre><code>    python -m venv tf_env\n</code></pre> step 3. Activate the environment  </p> <ul> <li>On CMD: <pre><code>    .\\tf_env\\Scripts\\activate\n</code></pre></li> <li>On PowerShell: <pre><code>    .\\tf_env\\Scripts\\Activate.ps1\n</code></pre> step 4. Upgrade pip &amp; install TensorFlow (with GPU support) <pre><code>pip install --upgrade pip\npip install tensorflow[and-cuda]\n</code></pre> <p>\u26a0\ufe0f This will install ~2.5 GB of GPU-enabled TensorFlow with pre-bundled CUDA &amp; cuDNN (no manual install needed in TF 2.15+).</p> </li> </ul>"},{"location":"chapter4_installation_and_setup/#42-verifying-installation-gpu-access","title":"4.2 Verifying Installation &amp; GPU Access","text":"<p>Create a file called <code>check_tf_gpu.py</code>: <pre><code>import tensorflow as tf\n\ndef print_gpu_info():\n    print(\"TensorFlow version:\", tf.__version__)\n    gpus = tf.config.list_physical_devices('GPU')\n    print(\"Num GPUs Available:\", len(gpus))\n    for gpu in gpus:\n        print(\"GPU Detected:\", gpu.name)\n\nif __name__ == '__main__':\n    print_gpu_info()\n</code></pre> Run it: <pre><code>python check_tf_gpu.py\n</code></pre></p> <p>\u2705 Expected Output: <pre><code>TensorFlow version: 2.x.x\nNum GPUs Available: 1\nGPU Detected: NVIDIA GeForce RTX 4050 Laptop GPU\n</code></pre> If it shows Num GPUs Available: 0, let\u2019s talk. We riot. (But also debug your drivers or reinstall with CPU-only fallback.)</p>"},{"location":"chapter4_installation_and_setup/#43-bonus-enable-dynamic-gpu-memory-growth","title":"4.3 Bonus: Enable Dynamic GPU Memory Growth","text":"<p>Prevent TensorFlow from hoarding all your GPU VRAM upfront: <pre><code>gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"Memory growth enabled on GPU.\")\n    except RuntimeError as e:\n        print(e)\n</code></pre> Use this in training scripts to allocate GPU memory only as needed.</p>"},{"location":"chapter4_installation_and_setup/#44-optional-freeze-your-environment","title":"4.4 Optional: Freeze Your Environment","text":"<p>To create a portable list of all packages: <pre><code>pip freeze &gt; requirements.txt\n</code></pre> Useful when sharing your book repo or collaborating with others.</p> <p>\u201cA neural net\u2019s journey begins with a single tensor.\u201d</p>"},{"location":"chapter5_Hello_tf_tensor/","title":"Chapter 5: Hello, tf.Tensor!","text":"<p>\u201cAll of machine learning boils down to manipulating tensors\u2014smartly.\u201d</p>"},{"location":"chapter5_Hello_tf_tensor/#51-what-is-a-tensor","title":"5.1 What is a Tensor?","text":"<p>Think of a tensor as a container for numbers, with a specific number of dimensions (aka rank).</p> <ul> <li>Scalar \u2192 Rank 0 \u2192 Just a number</li> <li>Vector \u2192 Rank 1 \u2192 A list of numbers</li> <li>Matrix \u2192 Rank 2 \u2192 Rows and columns</li> <li>Tensor \u2192 Rank \u22653 \u2192 Multi-dimensional data (e.g. images, videos, batches)</li> <li>Everything in TensorFlow revolves around these.</li> </ul>"},{"location":"chapter5_Hello_tf_tensor/#52-creating-tensors","title":"5.2 Creating Tensors","text":"<p>Let\u2019s create some: <pre><code>import tensorflow as tf\n\n# Scalar (Rank 0)\nscalar = tf.constant(42)\n\n# Vector (Rank 1)\nvector = tf.constant([1.0, 2.0, 3.0])\n\n# Matrix (Rank 2)\nmatrix = tf.constant([[1, 2], [3, 4]])\n\n# 3D Tensor\ntensor3d = tf.constant([[[1], [2]], [[3], [4]]])\n</code></pre> Use <code>tf.constant()</code> to create immutable tensors. Use <code>tf.Variable()</code> if you want a trainable version.</p>"},{"location":"chapter5_Hello_tf_tensor/#53-exploring-tensors","title":"5.3 Exploring Tensors","text":"<p>Let\u2019s peek inside: <pre><code>print(\"Shape:\", tensor3d.shape)\nprint(\"Rank (ndim):\", tf.rank(tensor3d))\nprint(\"DType:\", tensor3d.dtype)\nprint(\"Device:\", tensor3d.device)\n</code></pre> Output might look like: <pre><code>Shape: (2, 2, 1)\nRank (ndim): tf.Tensor(3, shape=(), dtype=int32)\nDType: &lt;dtype: 'int32'&gt;\n</code></pre></p>"},{"location":"chapter5_Hello_tf_tensor/#54-type-and-shape-manipulation","title":"5.4 Type and Shape Manipulation","text":"<p>TensorFlow is strict about data types and shapes\u2014get comfortable doing this: <pre><code># Cast to float32\nfloat_tensor = tf.cast(matrix, dtype=tf.float32)\n\n# Reshape (e.g. flatten 2x2 \u2192 4)\nreshaped = tf.reshape(matrix, [4])\n\n# Expand dimensions (useful for batch simulation)\nexpanded = tf.expand_dims(vector, axis=0)  # Now shape = (1, 3)\n\n# Squeeze dimensions\nsqueezed = tf.squeeze(tensor3d)\n</code></pre></p>"},{"location":"chapter5_Hello_tf_tensor/#55-basic-tensor-operations","title":"5.5 Basic Tensor Operations","text":"<pre><code>a = tf.constant([1, 2, 3])\nb = tf.constant([4, 5, 6])\n\n# Element-wise\nprint(tf.add(a, b))        # [5 7 9]\nprint(tf.multiply(a, b))   # [4 10 18]\n\n# Matrix multiplication\nmat1 = tf.constant([[1, 2], [3, 4]])\nmat2 = tf.constant([[5, 6], [7, 8]])\nprint(tf.matmul(mat1, mat2))  # [[19 22], [43 50]]\n</code></pre> <p>\ud83d\udca1 TensorFlow broadcasts shapes automatically if they align\u2014more on that in Chapter 7.</p>"},{"location":"chapter5_Hello_tf_tensor/#56-inspecting-and-debugging","title":"5.6 Inspecting and Debugging","text":"<p>For development, you\u2019ll often want to inspect: <pre><code>print(tf.shape(matrix))     # Tensor with shape info\nprint(matrix.numpy())       # Convert to NumPy for debugging\n</code></pre></p> <p>You can convert any tensor to a NumPy array using .numpy()\u2014especially useful when running in eager mode.</p> <p>\u201cAll of machine learning boils down to manipulating tensors\u2014smartly.\u201d</p>"},{"location":"chapter6_tensor_indexing_reshaping/","title":"Chapter 6: Tensor Indexing &amp; Reshaping","text":"<p>\u201cTensors may be infinite in dimension, but mastery begins with the first slice.\u201d</p>"},{"location":"chapter6_tensor_indexing_reshaping/#61-why-indexing-reshaping-matter","title":"6.1 Why Indexing &amp; Reshaping Matter","text":"<p>Before you train a single model, you\u2019ll spend a good chunk of time doing this:</p> <ul> <li>Selecting rows, columns, or channels</li> <li>Flattening or expanding shapes</li> <li>Swapping axes</li> <li>Prepping tensors for layers like Dense, Conv2D, or RNN</li> </ul> <p>Think of this as data martial arts\u2014getting your tensors into the right stance before the real fight begins.</p>"},{"location":"chapter6_tensor_indexing_reshaping/#62-basic-indexing-rank-1-and-2","title":"6.2 Basic Indexing (Rank 1 and 2)","text":"<p><pre><code>import tensorflow as tf\n\n# Rank 1 (vector)\nvec = tf.constant([10, 20, 30, 40])\nprint(vec[0])     # 10\nprint(vec[-1])    # 40\n\n# Rank 2 (matrix)\nmat = tf.constant([[1, 2], [3, 4], [5, 6]])\nprint(mat[1])        # [3, 4]\nprint(mat[1, 0])     # 3\n</code></pre> Slicing also works: <pre><code>print(mat[:, 0])     # First column: [1, 3, 5]\nprint(mat[0:2, :])   # First two rows: [[1, 2], [3, 4]]\n</code></pre></p>"},{"location":"chapter6_tensor_indexing_reshaping/#63-reshaping-tensors","title":"6.3 Reshaping Tensors","text":"<p><code>tf.reshape()</code> lets you change a tensor\u2019s shape without changing its data: <pre><code>x = tf.constant([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\nreshaped = tf.reshape(x, [3, 2])         # Shape: (3, 2)\nprint(reshaped)\n</code></pre> You can also flatten: <pre><code>flat = tf.reshape(x, [-1])  # Automatically infer length\nprint(flat)  # [1, 2, 3, 4, 5, 6]\n</code></pre></p>"},{"location":"chapter6_tensor_indexing_reshaping/#64-expanding-and-squeezing-dimensions","title":"6.4 Expanding and Squeezing Dimensions","text":"<p>These are crucial when batching data or feeding into specific layer shapes: <pre><code>x = tf.constant([1, 2, 3])  # Shape: (3,)\n\n# Expand\nx_expanded = tf.expand_dims(x, axis=0)  # Shape: (1, 3)\nx_expanded2 = tf.expand_dims(x, axis=1) # Shape: (3, 1)\n\n# Squeeze\nx_squeezed = tf.squeeze(tf.constant([[1], [2], [3]]))  # Shape: (3,)\n</code></pre> Use cases:</p> <ul> <li> <p>expand_dims \u2192 simulate a batch: [3] \u2192 [1, 3]</p> </li> <li> <p>squeeze \u2192 remove unnecessary dimensions (e.g. from model outputs)</p> </li> </ul>"},{"location":"chapter6_tensor_indexing_reshaping/#65-transposing-and-permuting-axes","title":"6.5 Transposing and Permuting Axes","text":"<p>You can swap dimensions using <code>tf.transpose()</code>: <pre><code>x = tf.constant([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\nprint(tf.transpose(x))  # Shape: (3, 2)\n</code></pre> For higher-rank tensors, use <code>perm</code>: <pre><code>x = tf.random.normal([2, 3, 4])\nx_transposed = tf.transpose(x, perm=[0, 2, 1])  # Swaps last two dims\n</code></pre></p>"},{"location":"chapter6_tensor_indexing_reshaping/#66-tensor-shape-tricks-youll-actually-use","title":"6.6 Tensor Shape Tricks You\u2019ll Actually Use","text":"Goal Command Flatten a tensor tf.reshape(tensor, [-1]) Add batch dimension tf.expand_dims(tensor, axis=0) Remove singleton dims tf.squeeze(tensor) Change channel-last to first tf.transpose(tensor, [0, 3, 1, 2]) Recover original shape tf.reshape(tensor, orig_shape)"},{"location":"chapter6_tensor_indexing_reshaping/#67-summary","title":"6.7 Summary","text":"<ul> <li>Indexing lets you extract elements, rows, columns, or slices from tensors of any rank.</li> <li>tf.reshape() allows you to safely change tensor shapes\u2014crucial before feeding into models.</li> <li>expand_dims() and squeeze() help manage batch dimensions and singleton axes.</li> <li>transpose() and perm are useful for rearranging axes, especially in image and sequence data.</li> <li>Shape manipulation is not just a utility\u2014it\u2019s how you adapt data to flow through deep learning systems.</li> </ul> <p>\u201cTensors may be infinite in dimension, but mastery begins with the first slice.\u201d</p>"},{"location":"chapter7_tensor_broadcasting/","title":"Chapter 7: Tensor Broadcasting","text":"<p>\u201cBroadcasting is TensorFlow\u2019s way of saying: \u2018Relax, I\u2019ve got this shape mismatch.\u2019\u201d</p>"},{"location":"chapter7_tensor_broadcasting/#71-what-is-broadcasting","title":"7.1 What is Broadcasting?","text":"<p>Broadcasting allows tensors with different shapes to participate in operations as if they had the same shape. It's like auto-expanding dimensions on the fly so that element-wise operations just work\u2014without explicitly reshaping anything. It\u2019s a core part of NumPy, PyTorch, and yes\u2014TensorFlow.</p>"},{"location":"chapter7_tensor_broadcasting/#72-broadcasting-in-action","title":"7.2 Broadcasting in Action","text":"<p>Let\u2019s take a practical example: <pre><code>import tensorflow as tf\n\na = tf.constant([[1, 2], [3, 4]])       # Shape: (2, 2)\nb = tf.constant([10, 20])               # Shape: (2,)\n\nresult = a + b\nprint(result)\n</code></pre> TensorFlow \u201cbroadcasts\u201d b from shape (2,) \u2192 (2, 2) by duplicating it across rows: <pre><code>[[1 + 10, 2 + 20],\n [3 + 10, 4 + 20]]\n= [[11, 22],\n   [13, 24]]\n</code></pre></p>"},{"location":"chapter7_tensor_broadcasting/#73-broadcasting-rules-the-intuition","title":"7.3 Broadcasting Rules (The Intuition)","text":"<p>TensorFlow compares dimensions from right to left:</p> <p>If same, they\u2019re compatible.</p> <p>If one is 1, it\u2019s stretched to match.</p> <p>If they don\u2019t match and neither is 1, it\u2019s an error. Example: <pre><code>a: (4, 1, 3)\nb: (  , 5, 1)\n</code></pre> \u2192 Resulting shape: <code>(4, 5, 3)</code> The middle <code>1</code> in <code>a</code> stretches to <code>5</code> The last <code>1</code> in <code>b</code> stretches to <code>3</code></p>"},{"location":"chapter7_tensor_broadcasting/#74-common-broadcasting-use-cases","title":"7.4 Common Broadcasting Use Cases","text":"<p>\u2705 Adding a bias vector to each row: <pre><code>x = tf.constant([[1, 2, 3], [4, 5, 6]])\nbias = tf.constant([10, 20, 30])\nprint(x + bias)\n</code></pre> \u2705 Multiplying by a scalar: <pre><code>x = tf.constant([[1.0, 2.0], [3.0, 4.0]])\nprint(x * 2.5)\n</code></pre> \u2705 Normalizing each feature (column-wise): <pre><code>x = tf.constant([[1., 2.], [3., 4.], [5., 6.]])\nmean = tf.reduce_mean(x, axis=0)  # Shape: (2,)\nprint(x - mean)  # Subtracts mean from each row\n</code></pre></p>"},{"location":"chapter7_tensor_broadcasting/#75-when-broadcasting-fails","title":"7.5 When Broadcasting Fails","text":"<p>Some operations won\u2019t broadcast if shapes are completely incompatible: <pre><code>a = tf.constant([1, 2, 3])      # Shape: (3,)\nb = tf.constant([[1, 2], [3, 4]])  # Shape: (2, 2)\n\n# tf.add(a, b) \u2192 \u274c Error: Shapes can't broadcast\n</code></pre> Always check shape compatibility first: <pre><code>print(\"Shape A:\", a.shape)\nprint(\"Shape B:\", b.shape)\n</code></pre> If needed, use:</p> <ul> <li> <p><code>tf.expand_dims()</code></p> </li> <li> <p><code>tf.reshape()</code></p> </li> <li> <p>or <code>tf.broadcast_to()</code> to explicitly adjust shapes</p> </li> </ul>"},{"location":"chapter7_tensor_broadcasting/#76-summary","title":"7.6 Summary","text":"<ul> <li>Broadcasting lets tensors with different shapes interact during operations.</li> <li>TensorFlow automatically stretches dimensions when one of them is 1.</li> <li>Most common use cases involve scalars, bias addition, and feature-wise normalization.</li> <li>Broadcasting removes boilerplate reshaping\u2014just mind your axes.</li> </ul> <p>\u201cBroadcasting is TensorFlow\u2019s way of saying: \u2018Relax, I\u2019ve got this shape mismatch.\u2019\u201d</p>"},{"location":"chapter8_ragged_sparse_string/","title":"Chapter 8: Ragged, Sparse, and String Tensors","text":"<p>\u201cNot all data fits in neat boxes. TensorFlow still makes it work.\u201d</p>"},{"location":"chapter8_ragged_sparse_string/#81-what-are-non-standard-tensors","title":"8.1 What Are Non-Standard Tensors?","text":"<p>Not all data comes in a clean matrix shape like [batch_size, features]. Real-world examples often include:  </p> <ul> <li>Sentences of different lengths (NLP)</li> <li>Feature vectors with missing values</li> <li>Text tokens, file paths, categorical strings</li> </ul> <p>Enter:  </p> <ul> <li>tf.RaggedTensor</li> <li>tf.SparseTensor</li> <li>tf.Tensor (with dtype=tf.string)</li> </ul>"},{"location":"chapter8_ragged_sparse_string/#82-ragged-tensors-for-variable-length-sequences","title":"8.2 Ragged Tensors \u2013 For Variable-Length Sequences","text":""},{"location":"chapter8_ragged_sparse_string/#use-case","title":"Use Case:","text":"<p>Think of sentences with different numbers of words: <pre><code>sentences = [\n    [\"Hello\", \"GPT-san\"],\n    [\"TensorFlow\"],\n    [\"Welcome\", \"to\", \"deep\", \"learning\"]\n]\n</code></pre></p>"},{"location":"chapter8_ragged_sparse_string/#code","title":"\u2705 Code:","text":"<pre><code>import tensorflow as tf\n\nrt = tf.ragged.constant([\n    [1, 2, 3],\n    [4, 5],\n    [6]\n])\n\nprint(rt)\nprint(\"Shape:\", rt.shape)\n</code></pre>"},{"location":"chapter8_ragged_sparse_string/#key-features","title":"Key Features:","text":"<ul> <li>Use .ragged_rank to inspect hierarchy  </li> <li>Can still use many standard ops like indexing, slicing  </li> <li>Great for tokenized text or nested lists</li> </ul>"},{"location":"chapter8_ragged_sparse_string/#83-sparse-tensors-for-efficiency-in-mostly-zero-data","title":"8.3 Sparse Tensors \u2013 For Efficiency in Mostly-Zero Data","text":""},{"location":"chapter8_ragged_sparse_string/#use-case_1","title":"Use Case:","text":"<p>When most values in a tensor are zero, storing all of them is wasteful. Use tf.SparseTensor to store just the non-zeros.</p> <p>\u2705 Code: <pre><code>st = tf.sparse.SparseTensor(\n    indices=[[0, 1], [1, 0]],\n    values=[10, 20],\n    dense_shape=[3, 3]\n)\n\ndense = tf.sparse.to_dense(st)\nprint(dense)\n</code></pre></p>"},{"location":"chapter8_ragged_sparse_string/#key-features_1","title":"Key Features:","text":"<ul> <li>Saves memory for large sparse data (e.g. recommender systems, one-hot vectors)</li> <li>Can convert to/from dense tensors</li> <li>Used heavily in embedding lookup and graph data</li> </ul>"},{"location":"chapter8_ragged_sparse_string/#84-string-tensors-for-text-data","title":"8.4 String Tensors \u2013 For Text Data","text":""},{"location":"chapter8_ragged_sparse_string/#use-case_2","title":"Use Case:","text":"<p>NLP often starts with raw strings\u2014TensorFlow supports them natively. <pre><code>str_tensor = tf.constant([\"Tensor\", \"Flow\", \"Rocks\"])\nprint(str_tensor)\nprint(tf.strings.length(str_tensor))       # Character length\nprint(tf.strings.upper(str_tensor))        # Uppercase conversion\nprint(tf.strings.join([str_tensor, \"!\"]))  # Add exclamations\n</code></pre></p>"},{"location":"chapter8_ragged_sparse_string/#key-features_2","title":"Key Features:","text":"<ul> <li>Native support for Unicode  </li> <li>Integrates with tf.strings, tf.text, and TextVectorization  </li> <li>First step before tokenization</li> </ul>"},{"location":"chapter8_ragged_sparse_string/#85-summary","title":"8.5 Summary","text":"<ul> <li>Ragged tensors store data with uneven lengths (e.g. variable-length sentences).</li> <li>Sparse tensors store only non-zero elements\u2014ideal for memory efficiency.</li> <li>String tensors enable natural language input processing natively.</li> <li>These types unlock real-world workflows where structure is messy or incomplete.</li> </ul> <p>\u201cNot all data fits in neat boxes. TensorFlow still makes it work.\u201d</p>"},{"location":"chapter9_variable_trainable_param/","title":"Chapter 9: Variables &amp; Trainable Parameters","text":"<p>\u201cA model learns by changing its variables\u2014not its mind.\u201d</p>"},{"location":"chapter9_variable_trainable_param/#91-what-is-a-tfvariable","title":"9.1 What is a <code>tf.Variable</code>?","text":"<p>While tf.Tensor is immutable, a <code>tf.Variable</code> is mutable\u2014its values can be updated during training.</p> <p>Think: - Weights - Biases - Embeddings</p> <p>All of these are backed by <code>tf.Variable</code>.</p>"},{"location":"chapter9_variable_trainable_param/#92-creating-variables","title":"9.2 Creating Variables","text":"<p>\u2705 Basic Example: <pre><code>import tensorflow as tf\n\n# Create a scalar variable\nw = tf.Variable(3.0)\n\n# Vector variable\nv = tf.Variable([1.0, 2.0, 3.0])\n</code></pre> You can inspect: <pre><code>print(\"Initial value:\", v.numpy())\nprint(\"Shape:\", v.shape)\nprint(\"Trainable:\", v.trainable)\n</code></pre></p>"},{"location":"chapter9_variable_trainable_param/#93-updating-variables","title":"9.3 Updating Variables","text":"<p>You can assign new values or add/subtract in-place: <pre><code>w.assign(5.0)\nw.assign_add(1.0)\nw.assign_sub(2.0)\nprint(w.numpy())  # Output: 4.0\n</code></pre></p> <p>This is what optimizers do under the hood: Update weights using gradients, via <code>assign_sub()</code>.</p>"},{"location":"chapter9_variable_trainable_param/#94-tfvariable-vs-tftensor","title":"9.4 tf.Variable vs tf.Tensor","text":"Feature <code>tf.Tensor</code> <code>tf.Variable</code> Immutable \u2705 Yes \u274c No Used for constants \u2705 Yes \ud83d\udeab Not recommended Learns in training \u274c No \u2705 Yes Used for model weights \u274c \u2705 Yes <p>Use <code>tf.Variable</code> when you want TensorFlow to track the state during training.</p>"},{"location":"chapter9_variable_trainable_param/#95-variables-in-custom-training-loops","title":"9.5 Variables in Custom Training Loops","text":"<p><pre><code>x = tf.Variable(2.0)\ny = tf.Variable(3.0)\n\nwith tf.GradientTape() as tape:\n    loss = x**2 + y**2\n\ngrads = tape.gradient(loss, [x, y])\nprint(\"Gradients:\", grads)\n\n# Manually update\nx.assign_sub(0.1 * grads[0])\ny.assign_sub(0.1 * grads[1])\n</code></pre> This mimics a single gradient descent step!</p>"},{"location":"chapter9_variable_trainable_param/#96-variable-collections-bonus","title":"9.6 Variable Collections (Bonus)","text":"<p>TensorFlow tracks variables using:</p> <ul> <li><code>tf.trainable_variables()</code></li> <li><code>model.trainable_variables</code> (in Keras models)</li> </ul> <p>This helps optimizers know what to update during <code>.fit()</code> or <code>apply_gradients</code>.</p>"},{"location":"chapter9_variable_trainable_param/#97-summary","title":"9.7 Summary","text":"<ul> <li><code>tf.Variable</code> stores trainable state\u2014used for weights, biases, and embeddings.  </li> <li>You can mutate them via <code>assign</code>, <code>assign_add</code>, and <code>assign_sub</code>.  </li> <li><code>tf.Tensor</code> is for static data, <code>tf.Variable</code> is for learning state.  </li> <li>They're essential for building models, training loops, and optimization.</li> </ul> <p>\u201cA model learns by changing its variables\u2014not its mind.\u201d</p>"}]}